{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avrymi-asraf/IDL-huji/blob/main/ex3/code_ex3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tools"
      ],
      "metadata": {
        "id": "ugToEoI5b_zH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "trjFyfsMSVIa"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "ddxV6kqJzhGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkvChoQRbSVT"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torchtext\n",
        "# !pip install torch\n",
        "# ! pip install pandas\n",
        "# ! pip install numpy\n",
        "!wget -O /content/IMDB_Dataset.csv https://raw.githubusercontent.com/avrymi-asraf/IDL-huji/main/ex3/IMDB%20Dataset.csv\n",
        "\n",
        "\n",
        "#https://raw.githubusercontent.com/avrymi-asraf/IDL-huji/main/ex3/IMDB%20Dataset.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ud0EYKh0Vf54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f326430d-c841-45db-d13b-efc216ecc0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import re\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown , clear_output, display\n",
        "\n",
        "md = lambda x: display(Markdown(x))\n",
        "he_md = lambda x: display(Markdown(f'<div dir=\"rtl\" lang=\"he\" xml:lang=\"he\">{x}</div>'))"
      ],
      "metadata": {
        "id": "aTHb8ODfd9_-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qClslSUJbK0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ea6c7a-51f2-4990-d772-a05d7060c02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:20<00:00, 19374.76it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MAX_LENGTH = 100\n",
        "embedding_size = 100\n",
        "Train_size=30000\n",
        "\n",
        "\n",
        "\n",
        "def review_clean(text):\n",
        "    text = re.sub(r'[^A-Za-z]+', ' ', text)  # remove non alphabetic character\n",
        "    text = re.sub(r'https?:/\\/\\S+', ' ', text)  # remove links\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)  # remove singale char\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def tokinize(s):\n",
        "    s = review_clean(s).lower()\n",
        "    splited = s.split()\n",
        "    return splited[:MAX_LENGTH]\n",
        "\n",
        "\n",
        "def load_data_set(load_my_reviews=False):\n",
        "    data=pd.read_csv(\"/content/IMDB_Dataset.csv\")\n",
        "    train_data=data[:Train_size]\n",
        "    train_iter=ReviewDataset(train_data[\"review\"],train_data[\"sentiment\"])\n",
        "    test_data=data[Train_size:]\n",
        "    if load_my_reviews:\n",
        "        my_data = pd.DataFrame({\"review\": my_test_texts, \"sentiment\": my_test_labels})\n",
        "        # test_data=test_data.append(my_data)\n",
        "        test_data = pd.concat([test_data, my_data], ignore_index=True)\n",
        "    test_data=test_data.reset_index(drop=True)\n",
        "    test_iter=ReviewDataset(test_data[\"review\"],test_data[\"sentiment\"])\n",
        "    return train_iter, test_iter\n",
        "\n",
        "\n",
        "embadding = GloVe(name='6B', dim=embedding_size)\n",
        "tokenizer = get_tokenizer(tokenizer=tokinize)\n",
        "\n",
        "\n",
        "def preprocess_review(s):\n",
        "    cleaned = tokinize(s)\n",
        "    embadded = embadding.get_vecs_by_tokens(cleaned)\n",
        "    if embadded.shape[0] != 100 or embadded.shape[1] != 100:\n",
        "        embadded = torch.nn.functional.pad(embadded, (0, 0, 0, MAX_LENGTH - embadded.shape[0]))\n",
        "    return torch.unsqueeze(embadded, 0)\n",
        "\n",
        "\n",
        "def preprocess_label(label):\n",
        "    return [0.0, 1.0] if label == \"negative\" else [1.0, 0.0]\n",
        "\n",
        "\n",
        "def collact_batch(batch):\n",
        "    label_list = []\n",
        "    review_list = []\n",
        "    embadding_list=[]\n",
        "    for  review,label in batch:\n",
        "        label_list.append(preprocess_label(label))### label\n",
        "        review_list.append(tokinize(review))### the  actuall review\n",
        "        processed_review = preprocess_review(review).detach()\n",
        "        embadding_list.append(processed_review) ### the embedding vectors\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32).reshape((-1, 2))\n",
        "    embadding_tensor= torch.cat(embadding_list)\n",
        "    return label_list.to(DEVICE), embadding_tensor.to(DEVICE) ,review_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, review_list, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.reviews = review_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.reviews[index]\n",
        "        y = self.labels[index]\n",
        "        return X, y\n",
        "\n",
        "\n",
        "\n",
        "def get_data_set(batch_size, toy=False):\n",
        "        train_data, test_data = load_data_set(load_my_reviews=toy)\n",
        "        train_dataloader = DataLoader(train_data, batch_size=batch_size,\n",
        "                                      shuffle=True, collate_fn=collact_batch)\n",
        "        test_dataloader = DataLoader(test_data, batch_size=batch_size,\n",
        "                                     shuffle=True, collate_fn=collact_batch)\n",
        "        return train_dataloader, test_dataloader, MAX_LENGTH, embedding_size\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title My review\n",
        "##########################\n",
        "# ADD YOUR OWN TEST TEXT #\n",
        "##########################\n",
        "\n",
        "my_test_texts = []\n",
        "my_test_texts.append(\" Never did I have such a good time in a movie in my whole life! Had such a good time playing on my smartphone the whole movie.\")\n",
        "my_test_texts.append(\" The movie was so captivating that I was completely absorbed in it, causing me to return home late to my wife, who was understandably upset that I lost track of time.\")\n",
        "my_test_texts.append(\"I was so engrossed in the movie's plot twists that I lost track of time and ended up missing an important appointment, bad luck.\")\n",
        "my_test_texts.append(\"Despite the low budget, the beautiful game covers up the lack of excessive props\")\n",
        "my_test_labels = [\"neg\", \"pos\", \"pos\",\"pos\"]\n",
        "# List of 100 ambiguous sentences that could be part of a movie review\n",
        "\n",
        "chatgpt_review = [\"great movie, really enjyoed it\",\n",
        "                 \"vary bad movie did not enjyoed it all\",\n",
        "                 \"outstanding highly recommended\",\n",
        "                 \"awful movie boring\",\n",
        "                 \"i didnt like the movie though the actors are great and talented\",\n",
        "                 \"how does such an outstanding actors and director produced this\",\n",
        "                 \"i liked it even though it is said to be boring and slow\"\n",
        "                 \"I enjoyed the movie\",\n",
        "\"I did not enjoy the movie\",\n",
        "\"Excellent movie\",\n",
        "\"I expected a good movie, I got a bad one\",\n",
        "\"A surprisingly bad plot\",\n",
        "\"Good plot\",\n",
        "\"A surprisingly good plot\",\n",
        "\"Good players!\",\n",
        "\"Despite the good actors, the movie is bad\",\n",
        "\"Bad players\",\n",
        "\"How do such good actors manage to make a really bad movie?\"\n",
        "                 ]\n",
        "chatgpt_labels = [\"neg\"] * len(chatgpt_review)\n",
        "\n",
        "\n",
        "\n",
        "##########################\n",
        "##########################\n",
        "create_my_list_dl = lambda: DataLoader(ReviewDataset(my_test_texts, my_test_labels), batch_size=len(my_test_texts), shuffle=False, collate_fn=collact_batch)\n",
        "chatgpt_dl = lambda: DataLoader(ReviewDataset(chatgpt_review, chatgpt_labels), batch_size=len(chatgpt_review), shuffle=False, collate_fn=collact_batch)"
      ],
      "metadata": {
        "id": "zdb7R2wW8PDH"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_positional_encoding(seq_len, d_model, device=\"cpu\"):\n",
        "    position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float, device=device) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    pe = torch.zeros(seq_len, d_model, device=device)\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, len_seq: int = 5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a long enough position tensor\n",
        "        position = torch.arange(len_seq).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Create the positional encoding matrix\n",
        "        pe = torch.zeros(len_seq, d_model)\n",
        "        pe[:,  0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe.squeeze_(1)\n",
        "\n",
        "        # Register the positional encoding as a buffer (not a parameter)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        return x + self.pe"
      ],
      "metadata": {
        "id": "_I3brr2J05CE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4wAuGAaHfc8B"
      },
      "outputs": [],
      "source": [
        "# Special matrix multipication layer (like torch.Linear but can operate on arbitrary sized\n",
        "# tensors and x its last two indices as the matrix.)\n",
        "\n",
        "class MatMul(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_bias = True):\n",
        "        super(MatMul, self).__init__()\n",
        "        self.matrix = torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(in_channels,out_channels)),requires_grad=True)\n",
        "        if use_bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(1,1,out_channels), requires_grad=True)\n",
        "\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.matmul(x,self.matrix)\n",
        "        if self.use_bias:\n",
        "            x = x+ self.bias\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Print review\n",
        "\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "import textwrap\n",
        "val = lambda x: \"Positive\" if x ==0 else \"Negative\"\n",
        "pred_val = lambda x: \"Positive\" if x[0] > x[1] else \"Negative\"\n",
        "# prints portion of the review (20-30 first words), with the sub-scores each work obtained\n",
        "# prints also the final scores, the softmaxed prediction values and the true label values\n",
        "\n",
        "def wrap(text, width=70):\n",
        "    # Split the text into paragraphs\n",
        "    paragraphs = text.split('\\n')\n",
        "\n",
        "    # Wrap each paragraph separately\n",
        "    wrapped_paragraphs = [\n",
        "        textwrap.fill(p, width=width, replace_whitespace=False)\n",
        "        for p in paragraphs\n",
        "    ]\n",
        "\n",
        "    # Join the wrapped paragraphs back together\n",
        "    return '\\n'.join(wrapped_paragraphs)\n",
        "\n",
        "def print_review(text, sbs, lbl):\n",
        "    \"\"\"\n",
        "    text:str the text of the review\n",
        "    sbs:Tensor(N,2)\n",
        "    lbl:Tensor(2)\n",
        "    \"\"\"\n",
        "    norm_sbs = sbs/sbs.max()\n",
        "    wrap_text = textwrap.fill(\" \".join(text), width=70)\n",
        "    word_and_score = \"  \".join(f\"{word}:{norm_sbs[i,0]:.2f},{norm_sbs[i,1]:.2f}\" for i, word in enumerate(text))\n",
        "    word_and_score = textwrap.fill(word_and_score, width=70)\n",
        "\n",
        "    prediction = sbs.mean(0)\n",
        "    print(f\"Review:\\n{wrap_text}\\n\\n\\n\",\n",
        "        f\"Word and score:(word:Pos,Neg)\\n{word_and_score}\\n\\n\\n\",\n",
        "        f\"Prediction: {pred_val(prediction)}, True label:{val(lbl)}, Prediction val(Positive:Negative){prediction[0].item():.3f}:{prediction[1].item():.3f}\\n\",\n",
        "        )\n",
        "\n",
        "\n",
        "def compare_model(mlp_model,attentoin_model,data_loader,only_diff=False):\n",
        "    out = open(\"results.md\", \"w\")\n",
        "    all_y,all_x,all_rev_text  = next(iter(data_loader))\n",
        "    mlp_model.eval()\n",
        "    attentoin_model.eval()\n",
        "    all_prd_mlp,all_sub_prd_mlp = mlp_model.sub_prediction(all_x)\n",
        "    all_prd_atten, all_sub_prd_atten, all_aw_atten = attentoin_model.sub_prediction(all_x)\n",
        "\n",
        "    all_prd_mlp = all_prd_mlp.detach().cpu()\n",
        "    all_sub_prd_mlp = all_sub_prd_mlp.detach().cpu()\n",
        "\n",
        "    all_prd_atten = all_prd_atten.detach().cpu()\n",
        "    all_sub_prd_atten = all_sub_prd_atten.detach().cpu()\n",
        "    all_aw_atten = all_aw_atten.detach().cpu()\n",
        "\n",
        "    all_y = all_y.detach().cpu()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(all_prd_mlp)):\n",
        "\n",
        "            prd_mlp = all_prd_mlp[i]\n",
        "            sub_prd_mlp = all_sub_prd_mlp[i]\n",
        "\n",
        "            prd_atten = all_prd_atten[i]\n",
        "            sub_prd_atten = all_sub_prd_atten[i]\n",
        "\n",
        "            if only_diff and pred_val(prd_mlp) == pred_val(prd_atten):\n",
        "                continue\n",
        "\n",
        "\n",
        "            rev_text = all_rev_text[i]\n",
        "            y = all_y[i][1]\n",
        "\n",
        "            hb_rev_text = translator.translate(\" \".join(rev_text),dest='he').text\n",
        "\n",
        "            n_sub_prd_mlp = (sub_prd_mlp/sub_prd_mlp.sum(dim=0))*10\n",
        "            n_sub_prd_atten = (sub_prd_atten/sub_prd_atten.sum(dim=0))*10\n",
        "\n",
        "            out_text = \"\"\n",
        "            out_text+=f\"# Text index: {i}\\n\\n\"\n",
        "            out_text+=f\"## True label: {val(y)}, attention prediction: ${prd_atten[0]:.3f}$ MLP prediction:${prd_mlp[0]:.3f}$\\n\"\n",
        "            out_text+= f\"### text:\\n{' '.join(rev_text)}\\n\"\n",
        "            out_text+= f'<div dir=\"rtl\" lang=\"he\">\\n\\n'\n",
        "            out_text+= f\"### text hebrow:\\n{hb_rev_text}\\n\"\n",
        "            out_text+= f'<div dir=\"ltr\" lang=\"en\">\\n\\n'\n",
        "\n",
        "\n",
        "            #mlp\n",
        "            word_and_score_mlp = \"  \".join(f\"{word}:_`{n_sub_prd_mlp[i,0]:.2f},{n_sub_prd_mlp[i,1]:.2f}`_\" for i, word in enumerate(rev_text))\n",
        "            out_text+= f\"### mlp word and score:\\n{word_and_score_mlp}\\n\"\n",
        "\n",
        "            #atten\n",
        "            word_and_score_atten = \"  \".join(f\"{word}:`{n_sub_prd_atten[i,0]:.2f},{n_sub_prd_atten[i,1]:.2f}`\" for i, word in enumerate(rev_text))\n",
        "            out_text+= f\"### attention word and score:\\n{word_and_score_atten}\\n\"\n",
        "\n",
        "\n",
        "            #both\n",
        "            out_text+=f\"### Attention and MLP score:\\n\"\n",
        "            word_and_score_atten = \"  \".join(f\"{word}:`{sub_prd_atten[i,0]-sub_prd_atten[i,1]:.2f},{sub_prd_mlp[i,0]-sub_prd_mlp[i,1]:.2f}`\" for i, word in enumerate(rev_text))\n",
        "            out_text+= f\"### attention word and score:\\n{word_and_score_atten}\\n\"\n",
        "\n",
        "            out.write(wrap(out_text, width=100))\n",
        "    out.close()"
      ],
      "metadata": {
        "id": "eOrwUNYkcSJY"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOeKPK18vhMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.rand(10,2)\n",
        "t = (t/t.sum(dim=0))*10\n",
        "t.sum(dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr-MpeBjvWM6",
        "outputId": "bea986ec-b7ad-46a5-dbc7-f0a2b142560b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10., 10.])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1\n",
        "run RNN and GRU"
      ],
      "metadata": {
        "id": "DaZ3SBI6aZVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfl_-2ILb7Ou",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title RNN Module\n",
        "class ExRNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExRNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sigmoid = torch.sigmoid\n",
        "        self.tanh = torch.tanh\n",
        "\n",
        "        # RNN Cell weights\n",
        "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size).to(DEVICE)\n",
        "        self.output_mlp = nn.Linear(hidden_size,output_size).to(DEVICE)\n",
        "\n",
        "    def name(self):\n",
        "        return \"RNN\"\n",
        "\n",
        "    def forward(self, x, hidden_state): #/1,time,(100,100)\n",
        "\n",
        "        x, hidden_state = x.to(DEVICE), hidden_state.to(DEVICE)\n",
        "        # Implementation of RNN cell\n",
        "        mid = torch.cat((x, hidden_state), 1)\n",
        "        hidden = self.tanh(self.in2hidden(mid))\n",
        "        output =self.output_mlp(hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        return torch.zeros(bs, self.hidden_size).to(DEVICE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wqkegnsb_Xl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title GRU Module\n",
        "\n",
        "class ExGRU(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExGRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sigmoid = torch.sigmoid\n",
        "        self.tanh = torch.tanh\n",
        "        self.update_gate = nn.Linear(input_size+hidden_size,hidden_size)\n",
        "        self.reset_gate = nn.Linear(input_size+hidden_size,hidden_size)\n",
        "        self.fc = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.output_mlp = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "    def name(self):\n",
        "        return \"GRU\"\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        x, hidden_state = x.to(DEVICE), hidden_state.to(DEVICE)\n",
        "        cat_tensor = torch.cat([hidden_state,x],dim=1)\n",
        "        zt = self.sigmoid(self.update_gate(cat_tensor))\n",
        "        rt = self.sigmoid(self.reset_gate(cat_tensor))\n",
        "        mid = hidden_state*rt\n",
        "        cat_mid = torch.cat([mid,x], dim=1)\n",
        "        h_hat = self.tanh(self.fc(cat_mid))\n",
        "        hidden = ((1-zt)*hidden_state) + (zt*h_hat)\n",
        "        output = self.output_mlp(hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        return torch.zeros(bs, self.hidden_size)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1xalwDpcKyn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run Function for RNN and GRU\n",
        "# select model to use\n",
        "\n",
        "def run_model(results, hidden_sizes, run_recurrent=True, use_RNN=True, reload_model=False, atten_size=0):\n",
        "    for hidden_size in hidden_sizes:\n",
        "      if run_recurrent:\n",
        "          if use_RNN:\n",
        "              model = ExRNN(input_size, output_size, hidden_size)\n",
        "          else:\n",
        "              model = ExGRU(input_size, output_size, hidden_size)\n",
        "      else:\n",
        "          print(\"Using MLP\")\n",
        "          return\n",
        "          # if atten_size > 0:\n",
        "          #     model = ExLRestSelfAtten(input_size, output_size, hidden_size)\n",
        "          # else:\n",
        "          #     model = ExMLP(input_size, output_size, hidden_size)\n",
        "\n",
        "      print(\"Using model: \" + model.name())\n",
        "\n",
        "      if reload_model:\n",
        "          print(\"Reloading model\")\n",
        "          model.load_state_dict(torch.load(model.name() + \".pth\"))\n",
        "\n",
        "      # move model to GPU if avilavble\n",
        "      model.to(DEVICE)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      train_loss = 1.0\n",
        "      test_loss = 1.0\n",
        "\n",
        "      train_losses = []\n",
        "      test_losses = []\n",
        "      accuracies = []\n",
        "\n",
        "      # training steps in which a test step is executed every test_interval\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "\n",
        "          itr = 0 # iteration counter within each epoch\n",
        "\n",
        "          for labels, reviews, reviews_text in train_dataset:   # getting training batches\n",
        "\n",
        "              itr = itr + 1\n",
        "\n",
        "              if (itr + 1) % test_interval == 0:\n",
        "                  test_iter = True\n",
        "                  labels, reviews, reviews_text = next(iter(test_dataset)) # get a test batch\n",
        "              else:\n",
        "                  test_iter = False\n",
        "\n",
        "              # Recurrent nets (RNN/GRU)\n",
        "\n",
        "              if run_recurrent:\n",
        "                  hidden_state = model.init_hidden(int(labels.shape[0]))\n",
        "\n",
        "                  for i in range(num_words):\n",
        "                      output, hidden_state = model(reviews[:,i,:], hidden_state)\n",
        "\n",
        "              else:\n",
        "\n",
        "              # Token-wise networks (MLP / MLP + Atten.)\n",
        "\n",
        "                  sub_score = []\n",
        "                  if atten_size > 0:\n",
        "                      # MLP + atten\n",
        "                      sub_score, atten_weights = model(reviews)\n",
        "                  else:\n",
        "                      # MLP\n",
        "                      sub_score = model(reviews)\n",
        "\n",
        "                  output = torch.mean(sub_score, 1)\n",
        "\n",
        "              print(f\"for testing: Output shape: {output.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "              # cross-entropy loss\n",
        "\n",
        "              loss = criterion(output, labels)\n",
        "\n",
        "              # optimize in training iterations\n",
        "\n",
        "              if not test_iter:\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "              # averaged losses\n",
        "              if test_iter:\n",
        "                  test_loss = 0.8 * float(loss.detach()) + 0.2 * test_loss\n",
        "              else:\n",
        "                  train_loss = 0.9 * float(loss.detach()) + 0.1 * train_loss\n",
        "\n",
        "              if test_iter:\n",
        "                train_losses.append(train_loss)\n",
        "                test_losses.append(test_loss)\n",
        "\n",
        "                #print(f\"Output shape: {output.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "                # Calculate accuracy\n",
        "                # Ensure the output and labels have compatible dimensions\n",
        "                #print(f\"Output shape before processing: {output.shape}\")\n",
        "                #print(f\"Labels shape before processing: {labels.shape}\")\n",
        "\n",
        "                if len(output.shape) > 1 and output.shape[1] > 1:\n",
        "                    _, predicted = torch.max(output, 1)\n",
        "                else:\n",
        "                    predicted = output\n",
        "\n",
        "                # Convert labels to class indices if they are not already\n",
        "                if len(labels.shape) > 1 and labels.shape[1] > 1:\n",
        "                    labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "                # Print shapes after processing\n",
        "                #print(f\"Output shape after processing: {output.shape}\")\n",
        "                #print(f\"Predicted shape: {predicted.shape}\")\n",
        "                #print(f\"Labels shape after processing: {labels.shape}\")\n",
        "\n",
        "                # Print values to debug\n",
        "                #print(f\"Output: {output}\")\n",
        "                #print(f\"Predicted: {predicted}\")\n",
        "                #print(f\"Labels: {labels}\")\n",
        "\n",
        "                correct = (predicted == labels).sum().item()\n",
        "                accuracy = correct / labels.size(0)\n",
        "                accuracies.append(accuracy)\n",
        "\n",
        "                # Print correct predictions and accuracy\n",
        "                #print(f\"Correct predictions: {correct}\")\n",
        "                #print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "                print(\n",
        "                      f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "                      f\"Step [{itr + 1}/{len(train_dataset)}], \"\n",
        "                      f\"Train Loss: {train_loss:.4f}, \"\n",
        "                      f\"Test Loss: {test_loss:.4f}\"\n",
        "                  )\n",
        "\n",
        "                if not run_recurrent:\n",
        "                    nump_subs = sub_score.detach().numpy()\n",
        "                    labels = labels.detach().numpy()\n",
        "                    print_review(reviews_text[0], nump_subs[0,:,0], nump_subs[0,:,1], labels[0,0], labels[0,1])\n",
        "\n",
        "                  # saving the model\n",
        "                torch.save(model, model.name() + \".pth\")\n",
        "      results[hidden_size] = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"accuracies\": accuracies\n",
        "      }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d7sPTl2bhde",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameters RNN and GRU\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "#hidden_size = 64        # to experiment with\n",
        "\n",
        "run_recurrent = False    # else run Token-wise MLP\n",
        "use_RNN = False         # otherwise GRU\n",
        "atten_size = 0          # atten > 0 means using restricted self atten\n",
        "\n",
        "reload_model = False\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "test_interval = 50\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGlTFP7ybn6i"
      },
      "outputs": [],
      "source": [
        "# Loading sataset, use toy = True for obtaining a smaller dataset\n",
        "train_dataset, test_dataset, num_words, input_size = get_data_set(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jeZSUwT8RLv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run recurrent and RGU\n",
        "results = {}\n",
        "hidden_sizes = [64\n",
        "                # , 128\n",
        "                ]\n",
        "run_model(results, hidden_sizes=hidden_sizes, run_recurrent=run_recurrent, use_RNN=use_RNN, reload_model=reload_model, atten_size=atten_size)\n",
        "for hidden_size, result in results.items():\n",
        "    plt.figure()\n",
        "    plt.plot(result['train_losses'], label='Train Loss')\n",
        "    plt.plot(result['test_losses'], label='Test Loss')\n",
        "    plt.plot(result['accuracies'], label='Accuracy')\n",
        "    plt.title(f'Performance with hidden size of{hidden_size}')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss/Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "Jm-oIuTlaO5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MLP Module\n",
        "class ExMLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExMLP, self).__init__()\n",
        "\n",
        "        self.ReLU = torch.nn.ReLU()\n",
        "\n",
        "        # Token-wise MLP network weights\n",
        "        self.layer1 = MatMul(input_size,hidden_size)\n",
        "        self.layer2 = MatMul(hidden_size,output_size)\n",
        "        # self.layer1 = nn.Linear(input_size,hidden_size) # (batch_size,hidden, 2 )\n",
        "        # self.layer2 = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "\n",
        "    def name(self):\n",
        "        return \"MLP\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: (batch_size, sequence, embedded)\n",
        "        '''\n",
        "\n",
        "        # Token-wise MLP network implementation\n",
        "        x = x.clone()\n",
        "        x = self.layer1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.ReLU(x) #(batch_size, seq,2)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = torch.softmax(x,dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def sub_prediction(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, sequence, embedded)\n",
        "        return prd, sub_prd\n",
        "        \"\"\"\n",
        "        x = x\n",
        "        x = self.layer1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.layer2(x)\n",
        "        sub_prd = self.ReLU(x).clone()\n",
        "        prd = torch.mean(sub_prd, dim=1)\n",
        "        prd = torch.softmax(prd,dim=1)\n",
        "        return prd, sub_prd\n",
        "\n"
      ],
      "metadata": {
        "id": "wQidLi3uaGki",
        "cellView": "form"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qA1iiGC-qHk3"
      },
      "outputs": [],
      "source": [
        "# @title Run Function for MLP\n",
        "\n",
        "def run_model_MLP(num_epochs, hidden_sizes,input_size,output_size, reload_model=False, attention=False,lr=1e-3,test_interval=50,print_examples_review=False):\n",
        "    results = {}\n",
        "    for hidden_size in hidden_sizes:\n",
        "        if attention:\n",
        "            model = ExLRestSelfAtten(input_size, output_size, hidden_size,5)\n",
        "        else:\n",
        "            model = ExMLP(input_size, output_size, hidden_size)\n",
        "\n",
        "        print(\"Using model: \" + model.name())\n",
        "\n",
        "        if reload_model:\n",
        "            print(\"Reloading model\")\n",
        "            model.load_state_dict(torch.load(model.name() + \".pth\"))\n",
        "\n",
        "        # move model to GPU if avilavble\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"triangular2\")\n",
        "\n",
        "        train_loss = 1.0\n",
        "        test_loss = 1.0\n",
        "\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "        accuracies = []\n",
        "\n",
        "      # training steps in which a test step is executed every test_interval\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for itr, (labels, reviews, reviews_text) in enumerate(train_dataset):   # getting training batches\n",
        "                labels = labels.to(DEVICE)\n",
        "                reviews = reviews.to(DEVICE)\n",
        "\n",
        "                output = model(reviews)\n",
        "                # output = torch.mean(sub_score, 1)\n",
        "                labels = torch.argmax(labels, dim=1).long()\n",
        "                # cross-entropy loss\n",
        "                loss = criterion(output, labels)\n",
        "                # optimize in training iterations\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                # averaged losses\n",
        "\n",
        "                train_loss = 0.7 * float(loss.detach()) + 0.3 * train_loss\n",
        "\n",
        "                if (itr + 1) % test_interval == 0:\n",
        "                    labels, reviews, reviews_text = next(iter(test_dataset)) # get a test batch:\n",
        "                    with torch.no_grad():\n",
        "                        labels = labels.to(DEVICE)\n",
        "                        reviews = reviews.to(DEVICE)\n",
        "\n",
        "                        if attention:\n",
        "                            # MLP + atten\n",
        "                            output,sub_prd,atten_weights = model.sub_prediction(reviews)\n",
        "                        else:\n",
        "                            # MLP\n",
        "                            output, sub_prd = model.sub_prediction(reviews)\n",
        "\n",
        "                        # output = torch.mean(sub_score, 1)\n",
        "                        labels = torch.argmax(labels, dim=1).long()\n",
        "                        loss = criterion(output, labels)\n",
        "                        test_loss = 0.7 * float(loss.detach()) + 0.3 * test_loss\n",
        "\n",
        "\n",
        "                    train_losses.append(train_loss)\n",
        "                    test_losses.append(test_loss)\n",
        "                    _, predicted = torch.max(output, 1)\n",
        "\n",
        "\n",
        "                    correct = (predicted == labels).sum().item()\n",
        "                    accuracy = correct / labels.size(0)\n",
        "                    accuracies.append(accuracy)\n",
        "\n",
        "                    # Print correct predictions and accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    print(f\"\"\"\n",
        "                    \\rHidden size: {hidden_size}\n",
        "                    \\rEpoch [{epoch + 1}/{num_epochs}]\n",
        "                    \\rStep [{itr + 1}/{len(train_dataset)}]\n",
        "                    \\rTrain Loss: {train_loss:.4f}\n",
        "                    \\rTest Loss: {test_loss:.4f}\n",
        "                    \\rCorrect predictions: [{correct}/{labels.size(0)}]\n",
        "                    \\rAccuracy: {accuracy}\"\"\")\n",
        "\n",
        "                    nump_subs = sub_prd.detach()\n",
        "                    # print(reviews_text[0],nump_subs[0],labels[0])\n",
        "                    if print_examples_review:\n",
        "                        print_review(reviews_text[0], nump_subs[0],labels[0])\n",
        "\n",
        "                    # saving the model\n",
        "                    torch.save(model, f'{model.name()}_{hidden_size}.pth')\n",
        "        results[hidden_size] = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"accuracies\": accuracies\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parameters MLP\n",
        "batch_size = 64\n",
        "input_size = 100\n",
        "output_size = 2\n",
        "atten_size = 0          # atten > 0 means using restricted self atten\n",
        "\n",
        "reload_model = False\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "QcV623oAZ7QS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading sataset, use toy = True for obtaining a smaller dataset\n",
        "train_dataset, test_dataset, num_words, input_size = get_data_set(batch_size)"
      ],
      "metadata": {
        "id": "2y7vv4_Dha3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title run\n",
        "results = {}\n",
        "hidden_sizes = [128]\n",
        "results = run_model_MLP(num_epochs, hidden_sizes,input_size,output_size)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xORWGl_yfuQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=results[32]['test_losses'], name='32 test'))\n",
        "fig.add_trace(go.Scatter(y=results[32]['train_losses'], name='32 train'))\n",
        "fig.add_trace(go.Scatter(y=results[64]['test_losses'], name='64 test'))\n",
        "fig.add_trace(go.Scatter(y=results[64]['train_losses'], name='64 train'))\n",
        "fig.add_trace(go.Scatter(y=results[128]['test_losses'], name='128 test'))\n",
        "fig.add_trace(go.Scatter(y=results[128]['train_losses'], name='128 train'))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Dkx30F4I95I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3:"
      ],
      "metadata": {
        "id": "9_TCja3_hMP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9gnyQDQncIG6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SelfAttention Module\n",
        "\n",
        "class ExLRestSelfAtten(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, atten_size):\n",
        "        super(ExLRestSelfAtten, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.atten_size = atten_size\n",
        "        self.sqrt_hidden_size = np.sqrt(float(hidden_size))\n",
        "        self.ReLU = torch.nn.ReLU()\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_size, 2*atten_size+1)\n",
        "\n",
        "        # Token-wise MLP + Restricted Attention network implementation\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_v = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def name(self):\n",
        "        return \"MLP_atten\"\n",
        "\n",
        "    def sub_prediction(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "        x = self.layer1(x)\n",
        "        x = self.ReLU(x)  # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Padding and rolling for restricted self-attention\n",
        "        padded = pad(x, (0, 0, self.atten_size, self.atten_size, 0, 0))\n",
        "        x_nei = torch.stack([torch.roll(padded, k, dims=1) for k in range(-self.atten_size, self.atten_size+1)], dim=2)\n",
        "        x_nei = x_nei[:, self.atten_size:-self.atten_size, :, :]\n",
        "        x_nei = self.positional_encoding(x_nei)\n",
        "\n",
        "        # Computing q, k, v\n",
        "        q = self.W_q(x_nei)\n",
        "        k = self.W_k(x_nei)\n",
        "        v = self.W_v(x_nei)\n",
        "\n",
        "        # Attention mechanism\n",
        "        d = torch.matmul(q, k.transpose(-2, -1)) / self.sqrt_hidden_size\n",
        "        aw = self.softmax(d)\n",
        "        x = torch.matmul(aw, v)\n",
        "        x = torch.mean(x, dim=2)\n",
        "\n",
        "        # Final MLP layer\n",
        "        sub_prd = self.mlp(x)\n",
        "        prd = torch.mean(sub_prd, dim=1)\n",
        "        prd = torch.softmax(prd, dim=-1)\n",
        "\n",
        "        return prd, sub_prd, aw\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sub_prediction(x)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parameters SelfAtten\n",
        "batch_size = 64\n",
        "input_size = 100\n",
        "output_size = 2\n",
        "atten_size = 0          # atten > 0 means using restricted self atten\n",
        "\n",
        "reload_model = False\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "Tk1Fycl3fX3L"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset, num_words, input_size = get_data_set(batch_size)"
      ],
      "metadata": {
        "id": "VEtHpdDU6YcP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title run SelfAtten\n",
        "results = {}\n",
        "hidden_sizes = [64,32,128]\n",
        "results = run_model_MLP(num_epochs, hidden_sizes,input_size,output_size,attention=True)"
      ],
      "metadata": {
        "id": "j3Dq8xuGgGwe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "635fd83a-6eea-4481-d6c1-e1b6856608ec"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model: MLP_atten\n",
            "\n",
            "Hidden size: 64\n",
            "Epoch [1/5]\n",
            "Step [50/469]\n",
            "Train Loss: 0.6931\n",
            "Test Loss: 0.7852\n",
            "Correct predictions: [30/64]\n",
            "Accuracy: 0.46875\n",
            "\n",
            "Hidden size: 64\n",
            "Epoch [1/5]\n",
            "Step [100/469]\n",
            "Train Loss: 0.6931\n",
            "Test Loss: 0.7208\n",
            "Correct predictions: [31/64]\n",
            "Accuracy: 0.484375\n",
            "\n",
            "Hidden size: 64\n",
            "Epoch [1/5]\n",
            "Step [150/469]\n",
            "Train Loss: 0.6931\n",
            "Test Loss: 0.7014\n",
            "Correct predictions: [32/64]\n",
            "Accuracy: 0.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-22dce2173781>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhidden_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model_MLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-e7c19682efd3>\u001b[0m in \u001b[0;36mrun_model_MLP\u001b[0;34m(num_epochs, hidden_sizes, input_size, output_size, reload_model, attention, lr, test_interval, print_examples_review)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# getting training batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-0b07f6c34255>\u001b[0m in \u001b[0;36mcollact_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m  \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mlabel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m### label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mreview_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokinize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m### the  actuall review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprocessed_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0membadding_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_review\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### the embedding vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0b07f6c34255>\u001b[0m in \u001b[0;36mtokinize\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokinize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0msplited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msplited\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0b07f6c34255>\u001b[0m in \u001b[0;36mreview_clean\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreview_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^A-Za-z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove non alphabetic character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'https?:/\\/\\S+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\s+[a-zA-Z]\\s+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove singale char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "# fig.add_trace(go.Scatter(y=results[128]['test_losses'], name='128 test'))\n",
        "# fig.add_trace(go.Scatter(y=results[128]['train_losses'], name='128 train'))\n",
        "# fig.add_trace(go.Scatter(y=results[64]['test_losses'], name='64 test'))\n",
        "# fig.add_trace(go.Scatter(y=results[64]['train_losses'], name='64 train'))\n",
        "fig.add_trace(go.Scatter(y=results[32]['test_losses'], name='32 test'))\n",
        "fig.add_trace(go.Scatter(y=results[32]['train_losses'], name='32 train'))\n",
        "# fig.add_trace(go.Scatter(y=results[16]['test_losses'], name='16 test'))\n",
        "# fig.add_trace(go.Scatter(y=results[16]['train_losses'], name='16 train'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "2CIteIrhgy_G",
        "outputId": "66ce7389-bfb4-4248-f58a-edf6d48d98da"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c94d7151-66bd-4e8a-8a87-3f912bd3fd57\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c94d7151-66bd-4e8a-8a87-3f912bd3fd57\")) {                    Plotly.newPlot(                        \"c94d7151-66bd-4e8a-8a87-3f912bd3fd57\",                        [{\"name\":\"32 test\",\"y\":[0.7868924856185913,0.7239450669288634,0.6814643399715423,0.6694712655782699,0.6501974587750434,0.6340303160283565,0.6405132096270799,0.6507185304540419,0.6109925986641056,0.6274552250299873,0.6126926274417049,0.6289332965409221,0.6220736700116235,0.6380643009489368,0.6066288101478097,0.6273671611488961,0.6131847448450656,0.5916506691963175,0.6268469823655693,0.619691352047241,0.6146565794937957,0.6146019136903628,0.6305652271508557,0.6551611665129478,0.6440445734035913,0.6529994712165577,0.6646751316157455,0.6261280105044685,0.6365672566154671,0.5903023307817897,0.582727403202585,0.6113642605420743,0.6126374766909121,0.5962254990840863,0.6179913100512299,0.6267794494900546,0.6302497304997874,0.5628719841695438,0.6559869383351984,0.6157686214022929,0.6153957754484741,0.619514450732229,0.5977303341935151,0.6268907176229379,0.6113817270628519],\"type\":\"scatter\"},{\"name\":\"32 train\",\"y\":[0.692436006379126,0.6926028005638805,0.6717294036074253,0.6614713104415609,0.6569823779249301,0.6418129988995469,0.6245342192813976,0.6241870023990442,0.6456734681830067,0.63582816005873,0.6215393994953253,0.6276983606666168,0.620267900386424,0.6323512222784278,0.6233033761060534,0.6368421832724055,0.5978866898193114,0.6602119388322978,0.6057654771792742,0.6348934629571513,0.6680509205988706,0.6094930495015671,0.6188556530399427,0.6616392916518399,0.5827857168387522,0.6119297724101703,0.6375957680977646,0.6148296213598834,0.6658729801540695,0.6140726929197872,0.6393271239002855,0.6325436050032457,0.6200944342040297,0.5935178592399373,0.6303734067280666,0.6048588359400902,0.5753663829008924,0.6493032420559037,0.6011738598966327,0.6319241432462126,0.6334933887318882,0.607115638769856,0.64503271404296,0.5709426165632567,0.6174633928554224],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c94d7151-66bd-4e8a-8a87-3f912bd3fd57');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compare SelfAttention model and Mlp model\n",
        "atten_m = torch.load(\"MLP_atten_32.pth\")\n",
        "mlp_m = torch.load(\"MLP_128.pth\")\n",
        "# train_dataset, test_dataset, num_words, input_size = get_data_set(400)\n",
        "my_text_dl = create_my_list_dl()\n",
        "gpt_dl = chatgpt_dl()\n",
        "compare_model(mlp_m,atten_m,gpt_dl)"
      ],
      "metadata": {
        "id": "yxVUU5tTTM9S"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_review[27]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HtXMhydwPDsv",
        "outputId": "c1ebbf26-3fdc-49f0-8593-55ca48faae3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The visual motifs are so frequent, I started seeing them in my breakfast cereal.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZvNXjb0THqV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}