{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avrymi-asraf/IDL-huji/blob/main/ex3/code_ex3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tools"
      ],
      "metadata": {
        "id": "ugToEoI5b_zH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trjFyfsMSVIa"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "ddxV6kqJzhGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkvChoQRbSVT"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torchtext\n",
        "# !pip install torch\n",
        "# ! pip install pandas\n",
        "# ! pip install numpy\n",
        "!wget -O /content/IMDB_Dataset.csv https://raw.githubusercontent.com/avrymi-asraf/IDL-huji/main/ex3/IMDB%20Dataset.csv\n",
        "\n",
        "\n",
        "#https://raw.githubusercontent.com/avrymi-asraf/IDL-huji/main/ex3/IMDB%20Dataset.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud0EYKh0Vf54"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import re\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown , clear_output, display\n",
        "\n",
        "md = lambda x: display(Markdown(x))\n",
        "he_md = lambda x: display(Markdown(f'<div dir=\"rtl\" lang=\"he\" xml:lang=\"he\">{x}</div>'))"
      ],
      "metadata": {
        "id": "aTHb8ODfd9_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qClslSUJbK0v"
      },
      "outputs": [],
      "source": [
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MAX_LENGTH = 100\n",
        "embedding_size = 100\n",
        "Train_size=30000\n",
        "\n",
        "\n",
        "\n",
        "def review_clean(text):\n",
        "    text = re.sub(r'[^A-Za-z]+', ' ', text)  # remove non alphabetic character\n",
        "    text = re.sub(r'https?:/\\/\\S+', ' ', text)  # remove links\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)  # remove singale char\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def tokinize(s):\n",
        "    s = review_clean(s).lower()\n",
        "    splited = s.split()\n",
        "    return splited[:MAX_LENGTH]\n",
        "\n",
        "\n",
        "def load_data_set(load_my_reviews=False):\n",
        "    data=pd.read_csv(\"/content/IMDB_Dataset.csv\")\n",
        "    train_data=data[:Train_size]\n",
        "    train_iter=ReviewDataset(train_data[\"review\"],train_data[\"sentiment\"])\n",
        "    test_data=data[Train_size:]\n",
        "    if load_my_reviews:\n",
        "        my_data = pd.DataFrame({\"review\": my_test_texts, \"sentiment\": my_test_labels})\n",
        "        # test_data=test_data.append(my_data)\n",
        "        test_data = pd.concat([test_data, my_data], ignore_index=True)\n",
        "    test_data=test_data.reset_index(drop=True)\n",
        "    test_iter=ReviewDataset(test_data[\"review\"],test_data[\"sentiment\"])\n",
        "    return train_iter, test_iter\n",
        "\n",
        "\n",
        "embadding = GloVe(name='6B', dim=embedding_size)\n",
        "tokenizer = get_tokenizer(tokenizer=tokinize)\n",
        "\n",
        "\n",
        "def preprocess_review(s):\n",
        "    cleaned = tokinize(s)\n",
        "    embadded = embadding.get_vecs_by_tokens(cleaned)\n",
        "    if embadded.shape[0] != 100 or embadded.shape[1] != 100:\n",
        "        embadded = torch.nn.functional.pad(embadded, (0, 0, 0, MAX_LENGTH - embadded.shape[0]))\n",
        "    return torch.unsqueeze(embadded, 0)\n",
        "\n",
        "\n",
        "def preprocess_label(label):\n",
        "    return [0.0, 1.0] if label == \"negative\" else [1.0, 0.0]\n",
        "\n",
        "\n",
        "def collact_batch(batch):\n",
        "    label_list = []\n",
        "    review_list = []\n",
        "    embadding_list=[]\n",
        "    for  review,label in batch:\n",
        "        label_list.append(preprocess_label(label))### label\n",
        "        review_list.append(tokinize(review))### the  actuall review\n",
        "        processed_review = preprocess_review(review).detach()\n",
        "        embadding_list.append(processed_review) ### the embedding vectors\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32).reshape((-1, 2))\n",
        "    embadding_tensor= torch.cat(embadding_list)\n",
        "    return label_list.to(DEVICE), embadding_tensor.to(DEVICE) ,review_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, review_list, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.reviews = review_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.reviews[index]\n",
        "        y = self.labels[index]\n",
        "        return X, y\n",
        "\n",
        "\n",
        "\n",
        "def get_data_set(batch_size, toy=False):\n",
        "        train_data, test_data = load_data_set(load_my_reviews=toy)\n",
        "        train_dataloader = DataLoader(train_data, batch_size=batch_size,\n",
        "                                      shuffle=True, collate_fn=collact_batch)\n",
        "        test_dataloader = DataLoader(test_data, batch_size=batch_size,\n",
        "                                     shuffle=True, collate_fn=collact_batch)\n",
        "        return train_dataloader, test_dataloader, MAX_LENGTH, embedding_size\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title My review\n",
        "##########################\n",
        "# ADD YOUR OWN TEST TEXT #\n",
        "##########################\n",
        "\n",
        "my_test_texts = []\n",
        "my_test_texts.append(\" Never did I have such a good time in a movie in my whole life! Had such a good time playing on my smartphone the whole movie.\")\n",
        "my_test_texts.append(\" The movie was so captivating that I was completely absorbed in it, causing me to return home late to my wife, who was understandably upset that I lost track of time.\")\n",
        "my_test_texts.append(\"I was so engrossed in the movie's plot twists that I lost track of time and ended up missing an important appointment, bad luck.\")\n",
        "my_test_texts.append(\"Despite the low budget, the beautiful game covers up the lack of excessive props\")\n",
        "my_test_labels = [\"neg\", \"pos\", \"pos\",\"pos\"]\n",
        "# List of 100 ambiguous sentences that could be part of a movie review\n",
        "\n",
        "chatgpt_review = [\"great movie, really enjyoed it\",\n",
        "                 \"vary bad movie did not enjyoed it all\",\n",
        "                 \"outstanding highly recommended\",\n",
        "                 \"awful movie boring\",\n",
        "                 \"i didnt like the movie though the actors are great and talented\",\n",
        "                 \"how does such an outstanding actors and director produced this\",\n",
        "                 \"i liked it even though it is said to be boring and slow\"\n",
        "                 \"I enjoyed the movie\",\n",
        "\"I did not enjoy the movie\",\n",
        "\"Excellent movie\",\n",
        "\"I expected a good movie, I got a bad one\",\n",
        "\"A surprisingly bad plot\",\n",
        "\"Good plot\",\n",
        "\"A surprisingly good plot\",\n",
        "\"Good players!\",\n",
        "\"Despite the good actors, the movie is bad\",\n",
        "\"Bad players\",\n",
        "\"How do such good actors manage to make a really bad movie?\"\n",
        "                 ]\n",
        "chatgpt_labels = [\"neg\"] * len(chatgpt_review)\n",
        "\n",
        "\n",
        "\n",
        "##########################\n",
        "##########################\n",
        "create_my_list_dl = lambda: DataLoader(ReviewDataset(my_test_texts, my_test_labels), batch_size=len(my_test_texts), shuffle=False, collate_fn=collact_batch)\n",
        "chatgpt_dl = lambda: DataLoader(ReviewDataset(chatgpt_review, chatgpt_labels), batch_size=len(chatgpt_review), shuffle=False, collate_fn=collact_batch)"
      ],
      "metadata": {
        "id": "zdb7R2wW8PDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_positional_encoding(seq_len, d_model, device=\"cpu\"):\n",
        "    position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float, device=device) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    pe = torch.zeros(seq_len, d_model, device=device)\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, len_seq: int = 5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a long enough position tensor\n",
        "        position = torch.arange(len_seq).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Create the positional encoding matrix\n",
        "        pe = torch.zeros(len_seq, d_model)\n",
        "        pe[:,  0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe.squeeze_(1)\n",
        "\n",
        "        # Register the positional encoding as a buffer (not a parameter)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        return x + self.pe"
      ],
      "metadata": {
        "id": "_I3brr2J05CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wAuGAaHfc8B"
      },
      "outputs": [],
      "source": [
        "# Special matrix multipication layer (like torch.Linear but can operate on arbitrary sized\n",
        "# tensors and x its last two indices as the matrix.)\n",
        "\n",
        "class MatMul(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_bias = True):\n",
        "        super(MatMul, self).__init__()\n",
        "        self.matrix = torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(in_channels,out_channels)),requires_grad=True)\n",
        "        if use_bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(1,1,out_channels), requires_grad=True)\n",
        "\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.matmul(x,self.matrix)\n",
        "        if self.use_bias:\n",
        "            x = x+ self.bias\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Print review\n",
        "\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "import textwrap\n",
        "val = lambda x: \"Positive\" if x ==0 else \"Negative\"\n",
        "pred_val = lambda x: \"Positive\" if x[0] > x[1] else \"Negative\"\n",
        "# prints portion of the review (20-30 first words), with the sub-scores each work obtained\n",
        "# prints also the final scores, the softmaxed prediction values and the true label values\n",
        "\n",
        "def wrap(text, width=70):\n",
        "    # Split the text into paragraphs\n",
        "    paragraphs = text.split('\\n')\n",
        "\n",
        "    # Wrap each paragraph separately\n",
        "    wrapped_paragraphs = [\n",
        "        textwrap.fill(p, width=width, replace_whitespace=False)\n",
        "        for p in paragraphs\n",
        "    ]\n",
        "\n",
        "    # Join the wrapped paragraphs back together\n",
        "    return '\\n'.join(wrapped_paragraphs)\n",
        "\n",
        "def print_review(text, sbs, lbl):\n",
        "    \"\"\"\n",
        "    text:str the text of the review\n",
        "    sbs:Tensor(N,2)\n",
        "    lbl:Tensor(2)\n",
        "    \"\"\"\n",
        "    norm_sbs = sbs/sbs.max()\n",
        "    wrap_text = textwrap.fill(\" \".join(text), width=70)\n",
        "    word_and_score = \"  \".join(f\"{word}:{norm_sbs[i,0]:.2f},{norm_sbs[i,1]:.2f}\" for i, word in enumerate(text))\n",
        "    word_and_score = textwrap.fill(word_and_score, width=70)\n",
        "\n",
        "    prediction = sbs.mean(0)\n",
        "    print(f\"Review:\\n{wrap_text}\\n\\n\\n\",\n",
        "        f\"Word and score:(word:Pos,Neg)\\n{word_and_score}\\n\\n\\n\",\n",
        "        f\"Prediction: {pred_val(prediction)}, True label:{val(lbl)}, Prediction val(Positive:Negative){prediction[0].item():.3f}:{prediction[1].item():.3f}\\n\",\n",
        "        )\n",
        "\n",
        "\n",
        "def compare_model(mlp_model,attentoin_model,data_loader,only_diff=False):\n",
        "    out = open(\"results.md\", \"w\")\n",
        "    all_y,all_x,all_rev_text  = next(iter(data_loader))\n",
        "    mlp_model.eval()\n",
        "    attentoin_model.eval()\n",
        "    all_prd_mlp,all_sub_prd_mlp = mlp_model.sub_prediction(all_x)\n",
        "    all_prd_atten, all_sub_prd_atten, all_aw_atten = attentoin_model.sub_prediction(all_x)\n",
        "\n",
        "    all_prd_mlp = all_prd_mlp.detach().cpu()\n",
        "    all_sub_prd_mlp = all_sub_prd_mlp.detach().cpu()\n",
        "\n",
        "    all_prd_atten = all_prd_atten.detach().cpu()\n",
        "    all_sub_prd_atten = all_sub_prd_atten.detach().cpu()\n",
        "    all_aw_atten = all_aw_atten.detach().cpu()\n",
        "\n",
        "    all_y = all_y.detach().cpu()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(all_prd_mlp)):\n",
        "\n",
        "            prd_mlp = all_prd_mlp[i]\n",
        "            sub_prd_mlp = all_sub_prd_mlp[i]\n",
        "\n",
        "            prd_atten = all_prd_atten[i]\n",
        "            sub_prd_atten = all_sub_prd_atten[i]\n",
        "\n",
        "            if only_diff and pred_val(prd_mlp) == pred_val(prd_atten):\n",
        "                continue\n",
        "\n",
        "\n",
        "            rev_text = all_rev_text[i]\n",
        "            y = all_y[i][1]\n",
        "\n",
        "            hb_rev_text = translator.translate(\" \".join(rev_text),dest='he').text\n",
        "\n",
        "            n_sub_prd_mlp = (sub_prd_mlp/sub_prd_mlp.sum(dim=0))*10\n",
        "            n_sub_prd_atten = (sub_prd_atten/sub_prd_atten.sum(dim=0))*10\n",
        "\n",
        "            out_text = \"\"\n",
        "            out_text+=f\"# Text index: {i}\\n\\n\"\n",
        "            out_text+=f\"## True label: {val(y)}, attention prediction: ${prd_atten[0]:.3f}$ MLP prediction:${prd_mlp[0]:.3f}$\\n\"\n",
        "            out_text+= f\"### text:\\n{' '.join(rev_text)}\\n\"\n",
        "            out_text+= f'<div dir=\"rtl\" lang=\"he\">\\n\\n'\n",
        "            out_text+= f\"### text hebrow:\\n{hb_rev_text}\\n\"\n",
        "            out_text+= f'<div dir=\"ltr\" lang=\"en\">\\n\\n'\n",
        "\n",
        "\n",
        "            #mlp\n",
        "            word_and_score_mlp = \"  \".join(f\"{word}:_`{n_sub_prd_mlp[i,0]:.2f},{n_sub_prd_mlp[i,1]:.2f}`_\" for i, word in enumerate(rev_text))\n",
        "            out_text+= f\"### mlp word and score:\\n{word_and_score_mlp}\\n\"\n",
        "\n",
        "            #atten\n",
        "            word_and_score_atten = \"  \".join(f\"{word}:`{n_sub_prd_atten[i,0]:.2f},{n_sub_prd_atten[i,1]:.2f}`\" for i, word in enumerate(rev_text))\n",
        "            out_text+= f\"### attention word and score:\\n{word_and_score_atten}\\n\"\n",
        "\n",
        "\n",
        "            #both\n",
        "            out_text+=f\"### Attention and MLP score:\\n\"\n",
        "            word_and_score_atten = \"  \".join(f\"{word}:`{sub_prd_atten[i,0]-sub_prd_atten[i,1]:.2f},{sub_prd_mlp[i,0]-sub_prd_mlp[i,1]:.2f}`\" for i, word in enumerate(rev_text))\n",
        "            out_text+= f\"### attention word and score:\\n{word_and_score_atten}\\n\"\n",
        "\n",
        "            out.write(wrap(out_text, width=100))\n",
        "    out.close()"
      ],
      "metadata": {
        "id": "eOrwUNYkcSJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOeKPK18vhMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.rand(10,2)\n",
        "t = (t/t.sum(dim=0))*10\n",
        "t.sum(dim=0)"
      ],
      "metadata": {
        "id": "pr-MpeBjvWM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1\n",
        "run RNN and GRU"
      ],
      "metadata": {
        "id": "DaZ3SBI6aZVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfl_-2ILb7Ou",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title RNN Module\n",
        "class ExRNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExRNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sigmoid = torch.sigmoid\n",
        "        self.tanh = torch.tanh\n",
        "\n",
        "        # RNN Cell weights\n",
        "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size).to(DEVICE)\n",
        "        self.output_mlp = nn.Linear(hidden_size,output_size).to(DEVICE)\n",
        "\n",
        "    def name(self):\n",
        "        return \"RNN\"\n",
        "\n",
        "    def forward(self, x, hidden_state): #/1,time,(100,100)\n",
        "\n",
        "        x, hidden_state = x.to(DEVICE), hidden_state.to(DEVICE)\n",
        "        # Implementation of RNN cell\n",
        "        mid = torch.cat((x, hidden_state), 1)\n",
        "        hidden = self.tanh(self.in2hidden(mid))\n",
        "        output =self.output_mlp(hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        return torch.zeros(bs, self.hidden_size).to(DEVICE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wqkegnsb_Xl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title GRU Module\n",
        "\n",
        "class ExGRU(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExGRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sigmoid = torch.sigmoid\n",
        "        self.tanh = torch.tanh\n",
        "        self.update_gate = nn.Linear(input_size+hidden_size,hidden_size)\n",
        "        self.reset_gate = nn.Linear(input_size+hidden_size,hidden_size)\n",
        "        self.fc = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.output_mlp = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "    def name(self):\n",
        "        return \"GRU\"\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        x, hidden_state = x.to(DEVICE), hidden_state.to(DEVICE)\n",
        "        cat_tensor = torch.cat([hidden_state,x],dim=1)\n",
        "        zt = self.sigmoid(self.update_gate(cat_tensor))\n",
        "        rt = self.sigmoid(self.reset_gate(cat_tensor))\n",
        "        mid = hidden_state*rt\n",
        "        cat_mid = torch.cat([mid,x], dim=1)\n",
        "        h_hat = self.tanh(self.fc(cat_mid))\n",
        "        hidden = ((1-zt)*hidden_state) + (zt*h_hat)\n",
        "        output = self.output_mlp(hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        return torch.zeros(bs, self.hidden_size)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1xalwDpcKyn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run Function for RNN and GRU\n",
        "# select model to use\n",
        "\n",
        "def run_model(results, hidden_sizes, run_recurrent=True, use_RNN=True, reload_model=False, atten_size=0):\n",
        "    for hidden_size in hidden_sizes:\n",
        "      if run_recurrent:\n",
        "          if use_RNN:\n",
        "              model = ExRNN(input_size, output_size, hidden_size)\n",
        "          else:\n",
        "              model = ExGRU(input_size, output_size, hidden_size)\n",
        "      else:\n",
        "          print(\"Using MLP\")\n",
        "          return\n",
        "          # if atten_size > 0:\n",
        "          #     model = ExLRestSelfAtten(input_size, output_size, hidden_size)\n",
        "          # else:\n",
        "          #     model = ExMLP(input_size, output_size, hidden_size)\n",
        "\n",
        "      print(\"Using model: \" + model.name())\n",
        "\n",
        "      if reload_model:\n",
        "          print(\"Reloading model\")\n",
        "          model.load_state_dict(torch.load(model.name() + \".pth\"))\n",
        "\n",
        "      # move model to GPU if avilavble\n",
        "      model.to(DEVICE)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      train_loss = 1.0\n",
        "      test_loss = 1.0\n",
        "\n",
        "      train_losses = []\n",
        "      test_losses = []\n",
        "      accuracies = []\n",
        "\n",
        "      # training steps in which a test step is executed every test_interval\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "\n",
        "          itr = 0 # iteration counter within each epoch\n",
        "\n",
        "          for labels, reviews, reviews_text in train_dataset:   # getting training batches\n",
        "\n",
        "              itr = itr + 1\n",
        "\n",
        "              if (itr + 1) % test_interval == 0:\n",
        "                  test_iter = True\n",
        "                  labels, reviews, reviews_text = next(iter(test_dataset)) # get a test batch\n",
        "              else:\n",
        "                  test_iter = False\n",
        "\n",
        "              # Recurrent nets (RNN/GRU)\n",
        "\n",
        "              if run_recurrent:\n",
        "                  hidden_state = model.init_hidden(int(labels.shape[0]))\n",
        "\n",
        "                  for i in range(num_words):\n",
        "                      output, hidden_state = model(reviews[:,i,:], hidden_state)\n",
        "\n",
        "              else:\n",
        "\n",
        "              # Token-wise networks (MLP / MLP + Atten.)\n",
        "\n",
        "                  sub_score = []\n",
        "                  if atten_size > 0:\n",
        "                      # MLP + atten\n",
        "                      sub_score, atten_weights = model(reviews)\n",
        "                  else:\n",
        "                      # MLP\n",
        "                      sub_score = model(reviews)\n",
        "\n",
        "                  output = torch.mean(sub_score, 1)\n",
        "\n",
        "              print(f\"for testing: Output shape: {output.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "              # cross-entropy loss\n",
        "\n",
        "              loss = criterion(output, labels)\n",
        "\n",
        "              # optimize in training iterations\n",
        "\n",
        "              if not test_iter:\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "              # averaged losses\n",
        "              if test_iter:\n",
        "                  test_loss = 0.8 * float(loss.detach()) + 0.2 * test_loss\n",
        "              else:\n",
        "                  train_loss = 0.9 * float(loss.detach()) + 0.1 * train_loss\n",
        "\n",
        "              if test_iter:\n",
        "                train_losses.append(train_loss)\n",
        "                test_losses.append(test_loss)\n",
        "\n",
        "                #print(f\"Output shape: {output.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "                # Calculate accuracy\n",
        "                # Ensure the output and labels have compatible dimensions\n",
        "                #print(f\"Output shape before processing: {output.shape}\")\n",
        "                #print(f\"Labels shape before processing: {labels.shape}\")\n",
        "\n",
        "                if len(output.shape) > 1 and output.shape[1] > 1:\n",
        "                    _, predicted = torch.max(output, 1)\n",
        "                else:\n",
        "                    predicted = output\n",
        "\n",
        "                # Convert labels to class indices if they are not already\n",
        "                if len(labels.shape) > 1 and labels.shape[1] > 1:\n",
        "                    labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "                # Print shapes after processing\n",
        "                #print(f\"Output shape after processing: {output.shape}\")\n",
        "                #print(f\"Predicted shape: {predicted.shape}\")\n",
        "                #print(f\"Labels shape after processing: {labels.shape}\")\n",
        "\n",
        "                # Print values to debug\n",
        "                #print(f\"Output: {output}\")\n",
        "                #print(f\"Predicted: {predicted}\")\n",
        "                #print(f\"Labels: {labels}\")\n",
        "\n",
        "                correct = (predicted == labels).sum().item()\n",
        "                accuracy = correct / labels.size(0)\n",
        "                accuracies.append(accuracy)\n",
        "\n",
        "                # Print correct predictions and accuracy\n",
        "                #print(f\"Correct predictions: {correct}\")\n",
        "                #print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "                print(\n",
        "                      f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "                      f\"Step [{itr + 1}/{len(train_dataset)}], \"\n",
        "                      f\"Train Loss: {train_loss:.4f}, \"\n",
        "                      f\"Test Loss: {test_loss:.4f}\"\n",
        "                  )\n",
        "\n",
        "                if not run_recurrent:\n",
        "                    nump_subs = sub_score.detach().numpy()\n",
        "                    labels = labels.detach().numpy()\n",
        "                    print_review(reviews_text[0], nump_subs[0,:,0], nump_subs[0,:,1], labels[0,0], labels[0,1])\n",
        "\n",
        "                  # saving the model\n",
        "                torch.save(model, model.name() + \".pth\")\n",
        "      results[hidden_size] = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"accuracies\": accuracies\n",
        "      }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d7sPTl2bhde",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Parameters RNN and GRU\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "#hidden_size = 64        # to experiment with\n",
        "\n",
        "run_recurrent = False    # else run Token-wise MLP\n",
        "use_RNN = False         # otherwise GRU\n",
        "atten_size = 0          # atten > 0 means using restricted self atten\n",
        "\n",
        "reload_model = False\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "test_interval = 50\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGlTFP7ybn6i"
      },
      "outputs": [],
      "source": [
        "# Loading sataset, use toy = True for obtaining a smaller dataset\n",
        "train_dataset, test_dataset, num_words, input_size = get_data_set(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jeZSUwT8RLv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run recurrent and RGU\n",
        "results = {}\n",
        "hidden_sizes = [64\n",
        "                # , 128\n",
        "                ]\n",
        "run_model(results, hidden_sizes=hidden_sizes, run_recurrent=run_recurrent, use_RNN=use_RNN, reload_model=reload_model, atten_size=atten_size)\n",
        "for hidden_size, result in results.items():\n",
        "    plt.figure()\n",
        "    plt.plot(result['train_losses'], label='Train Loss')\n",
        "    plt.plot(result['test_losses'], label='Test Loss')\n",
        "    plt.plot(result['accuracies'], label='Accuracy')\n",
        "    plt.title(f'Performance with hidden size of{hidden_size}')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss/Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "Jm-oIuTlaO5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MLP Module\n",
        "class ExMLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExMLP, self).__init__()\n",
        "\n",
        "        self.ReLU = torch.nn.ReLU()\n",
        "\n",
        "        # Token-wise MLP network weights\n",
        "        self.layer1 = MatMul(input_size,hidden_size)\n",
        "        self.layer2 = MatMul(hidden_size,output_size)\n",
        "        # self.layer1 = nn.Linear(input_size,hidden_size) # (batch_size,hidden, 2 )\n",
        "        # self.layer2 = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "\n",
        "    def name(self):\n",
        "        return \"MLP\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: (batch_size, sequence, embedded)\n",
        "        '''\n",
        "\n",
        "        # Token-wise MLP network implementation\n",
        "        x = x.clone()\n",
        "        x = self.layer1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.ReLU(x) #(batch_size, seq,2)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = torch.softmax(x,dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def sub_prediction(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, sequence, embedded)\n",
        "        return prd, sub_prd\n",
        "        \"\"\"\n",
        "        x = x\n",
        "        x = self.layer1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.layer2(x)\n",
        "        sub_prd = self.ReLU(x).clone()\n",
        "        prd = torch.mean(sub_prd, dim=1)\n",
        "        prd = torch.softmax(prd,dim=1)\n",
        "        return prd, sub_prd\n",
        "\n"
      ],
      "metadata": {
        "id": "wQidLi3uaGki",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA1iiGC-qHk3"
      },
      "outputs": [],
      "source": [
        "# @title Run Function for MLP\n",
        "\n",
        "def run_model_MLP(num_epochs, hidden_sizes,input_size,output_size, reload_model=False, attention=False,lr=1e-3,test_interval=50,print_examples_review=False):\n",
        "    results = {}\n",
        "    for hidden_size in hidden_sizes:\n",
        "        if attention:\n",
        "            model = ExLRestSelfAtten(input_size, output_size, hidden_size,5)\n",
        "        else:\n",
        "            model = ExMLP(input_size, output_size, hidden_size)\n",
        "\n",
        "        print(\"Using model: \" + model.name())\n",
        "\n",
        "        if reload_model:\n",
        "            print(\"Reloading model\")\n",
        "            model.load_state_dict(torch.load(model.name() + \".pth\"))\n",
        "\n",
        "        # move model to GPU if avilavble\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"triangular2\")\n",
        "\n",
        "        train_loss = 1.0\n",
        "        test_loss = 1.0\n",
        "\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "        accuracies = []\n",
        "\n",
        "      # training steps in which a test step is executed every test_interval\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for itr, (labels, reviews, reviews_text) in enumerate(train_dataset):   # getting training batches\n",
        "                labels = labels.to(DEVICE)\n",
        "                reviews = reviews.to(DEVICE)\n",
        "\n",
        "                output = model(reviews)\n",
        "                # output = torch.mean(sub_score, 1)\n",
        "                labels = torch.argmax(labels, dim=1).long()\n",
        "                # cross-entropy loss\n",
        "                loss = criterion(output, labels)\n",
        "                # optimize in training iterations\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                # averaged losses\n",
        "\n",
        "                train_loss = 0.7 * float(loss.detach()) + 0.3 * train_loss\n",
        "\n",
        "                if (itr + 1) % test_interval == 0:\n",
        "                    labels, reviews, reviews_text = next(iter(test_dataset)) # get a test batch:\n",
        "                    with torch.no_grad():\n",
        "                        labels = labels.to(DEVICE)\n",
        "                        reviews = reviews.to(DEVICE)\n",
        "\n",
        "                        if attention:\n",
        "                            # MLP + atten\n",
        "                            output,sub_prd,atten_weights = model.sub_prediction(reviews)\n",
        "                        else:\n",
        "                            # MLP\n",
        "                            output, sub_prd = model.sub_prediction(reviews)\n",
        "\n",
        "                        # output = torch.mean(sub_score, 1)\n",
        "                        labels = torch.argmax(labels, dim=1).long()\n",
        "                        loss = criterion(output, labels)\n",
        "                        test_loss = 0.7 * float(loss.detach()) + 0.3 * test_loss\n",
        "\n",
        "\n",
        "                    train_losses.append(train_loss)\n",
        "                    test_losses.append(test_loss)\n",
        "                    _, predicted = torch.max(output, 1)\n",
        "\n",
        "\n",
        "                    correct = (predicted == labels).sum().item()\n",
        "                    accuracy = correct / labels.size(0)\n",
        "                    accuracies.append(accuracy)\n",
        "\n",
        "                    # Print correct predictions and accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    print(f\"\"\"\n",
        "                    \\rHidden size: {hidden_size}\n",
        "                    \\rEpoch [{epoch + 1}/{num_epochs}]\n",
        "                    \\rStep [{itr + 1}/{len(train_dataset)}]\n",
        "                    \\rTrain Loss: {train_loss:.4f}\n",
        "                    \\rTest Loss: {test_loss:.4f}\n",
        "                    \\rCorrect predictions: [{correct}/{labels.size(0)}]\n",
        "                    \\rAccuracy: {accuracy}\"\"\")\n",
        "\n",
        "                    nump_subs = sub_prd.detach()\n",
        "                    # print(reviews_text[0],nump_subs[0],labels[0])\n",
        "                    if print_examples_review:\n",
        "                        print_review(reviews_text[0], nump_subs[0],labels[0])\n",
        "\n",
        "                    # saving the model\n",
        "                    torch.save(model, f'{model.name()}_{hidden_size}.pth')\n",
        "        results[hidden_size] = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"accuracies\": accuracies\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parameters MLP\n",
        "batch_size = 64\n",
        "input_size = 100\n",
        "output_size = 2\n",
        "atten_size = 0          # atten > 0 means using restricted self atten\n",
        "\n",
        "reload_model = False\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "QcV623oAZ7QS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading sataset, use toy = True for obtaining a smaller dataset\n",
        "train_dataset, test_dataset, num_words, input_size = get_data_set(batch_size)"
      ],
      "metadata": {
        "id": "2y7vv4_Dha3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title run\n",
        "results = {}\n",
        "hidden_sizes = [128]\n",
        "results = run_model_MLP(num_epochs, hidden_sizes,input_size,output_size)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xORWGl_yfuQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=results[32]['test_losses'], name='32 test'))\n",
        "fig.add_trace(go.Scatter(y=results[32]['train_losses'], name='32 train'))\n",
        "fig.add_trace(go.Scatter(y=results[64]['test_losses'], name='64 test'))\n",
        "fig.add_trace(go.Scatter(y=results[64]['train_losses'], name='64 train'))\n",
        "fig.add_trace(go.Scatter(y=results[128]['test_losses'], name='128 test'))\n",
        "fig.add_trace(go.Scatter(y=results[128]['train_losses'], name='128 train'))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Dkx30F4I95I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3:"
      ],
      "metadata": {
        "id": "9_TCja3_hMP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gnyQDQncIG6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title SelfAttention Module\n",
        "\n",
        "class ExLRestSelfAtten(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, atten_size):\n",
        "        super(ExLRestSelfAtten, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.atten_size = atten_size\n",
        "        self.sqrt_hidden_size = np.sqrt(float(hidden_size))\n",
        "        self.ReLU = torch.nn.ReLU()\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_size, 2*atten_size+1)\n",
        "\n",
        "        # Token-wise MLP + Restricted Attention network implementation\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.W_v = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def name(self):\n",
        "        return \"MLP_atten\"\n",
        "\n",
        "    def sub_prediction(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_size)\n",
        "        x = self.layer1(x)\n",
        "        x = self.ReLU(x)  # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Padding and rolling for restricted self-attention\n",
        "        padded = pad(x, (0, 0, self.atten_size, self.atten_size, 0, 0))\n",
        "        x_nei = torch.stack([torch.roll(padded, k, dims=1) for k in range(-self.atten_size, self.atten_size+1)], dim=2)\n",
        "        x_nei = x_nei[:, self.atten_size:-self.atten_size, :, :]\n",
        "        x_nei = self.positional_encoding(x_nei)\n",
        "\n",
        "        # Computing q, k, v\n",
        "        q = self.W_q(x_nei)\n",
        "        k = self.W_k(x_nei)\n",
        "        v = self.W_v(x_nei)\n",
        "\n",
        "        # Attention mechanism\n",
        "        d = torch.matmul(q, k.transpose(-2, -1)) / self.sqrt_hidden_size\n",
        "        aw = self.softmax(d)\n",
        "        x = torch.matmul(aw, v)\n",
        "        x = torch.mean(x, dim=2)\n",
        "\n",
        "        # Final MLP layer\n",
        "        sub_prd = self.mlp(x)\n",
        "        prd = torch.mean(sub_prd, dim=1)\n",
        "        prd = torch.softmax(prd, dim=-1)\n",
        "\n",
        "        return prd, sub_prd, aw\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sub_prediction(x)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parameters SelfAtten\n",
        "batch_size = 64\n",
        "input_size = 100\n",
        "output_size = 2\n",
        "atten_size = 0          # atten > 0 means using restricted self atten\n",
        "\n",
        "reload_model = False\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "Tk1Fycl3fX3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset, num_words, input_size = get_data_set(batch_size)"
      ],
      "metadata": {
        "id": "VEtHpdDU6YcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title run SelfAtten\n",
        "results = {}\n",
        "hidden_sizes = [64,32,128]\n",
        "results = run_model_MLP(num_epochs, hidden_sizes,input_size,output_size,attention=True)"
      ],
      "metadata": {
        "id": "j3Dq8xuGgGwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "# fig.add_trace(go.Scatter(y=results[128]['test_losses'], name='128 test'))\n",
        "# fig.add_trace(go.Scatter(y=results[128]['train_losses'], name='128 train'))\n",
        "# fig.add_trace(go.Scatter(y=results[64]['test_losses'], name='64 test'))\n",
        "# fig.add_trace(go.Scatter(y=results[64]['train_losses'], name='64 train'))\n",
        "fig.add_trace(go.Scatter(y=results[32]['test_losses'], name='32 test'))\n",
        "fig.add_trace(go.Scatter(y=results[32]['train_losses'], name='32 train'))\n",
        "# fig.add_trace(go.Scatter(y=results[16]['test_losses'], name='16 test'))\n",
        "# fig.add_trace(go.Scatter(y=results[16]['train_losses'], name='16 train'))"
      ],
      "metadata": {
        "id": "2CIteIrhgy_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compare SelfAttention model and Mlp model\n",
        "atten_m = torch.load(\"MLP_atten_32.pth\")\n",
        "mlp_m = torch.load(\"MLP_128.pth\")\n",
        "# train_dataset, test_dataset, num_words, input_size = get_data_set(400)\n",
        "my_text_dl = create_my_list_dl()\n",
        "gpt_dl = chatgpt_dl()\n",
        "compare_model(mlp_m,atten_m,gpt_dl)"
      ],
      "metadata": {
        "id": "yxVUU5tTTM9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}