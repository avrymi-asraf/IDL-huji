{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avrymi-asraf/IDL-huji/blob/main/ex3_IDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "trjFyfsMSVIa"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkvChoQRbSVT",
        "outputId": "7131ee81-6986-4d85-fab1-cc344434e071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m--2024-07-04 11:25:14--  https://raw.githubusercontent.com/avrymi-asraf/IDL-huji/main/ex3/IMDB%20Dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66212309 (63M) [text/plain]\n",
            "Saving to: ‘/content/IMDB_Dataset.csv’\n",
            "\n",
            "/content/IMDB_Datas 100%[===================>]  63.14M  72.6MB/s    in 0.9s    \n",
            "\n",
            "2024-07-04 11:25:21 (72.6 MB/s) - ‘/content/IMDB_Dataset.csv’ saved [66212309/66212309]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torchtext\n",
        "# !pip install torch\n",
        "# ! pip install pandas\n",
        "# ! pip install numpy\n",
        "!wget -O /content/IMDB_Dataset.csv https://raw.githubusercontent.com/avrymi-asraf/IDL-huji/main/ex3/IMDB%20Dataset.csv\n",
        "\n",
        "\n",
        "#https://raw.githubusercontent.com/avrymi-asraf/IDL-huji/main/ex3/IMDB%20Dataset.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ud0EYKh0Vf54",
        "outputId": "db1ae07e-44ca-47ee-b6ec-5e21bdbe9a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# from torchtext.legacy.data import Field\n",
        "import torchtext as tx\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import re\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch as tr\n",
        "from torch.nn.functional import pad\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9d7sPTl2bhde"
      },
      "outputs": [],
      "source": [
        "########################################################################\n",
        "########################################################################\n",
        "##\n",
        "##                     BEST TEAM EVER\n",
        "                                            ##                     ##\n",
        "##                                                                    ##\n",
        "########################################################################\n",
        "########################################################################\n",
        "\n",
        "\n",
        "# import loader as ld\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "output_size = 1  # should be 2 for RNN and GRU\n",
        "#hidden_size = 64        # to experiment with\n",
        "\n",
        "run_recurrent = False    # else run Token-wise MLP\n",
        "use_RNN = False         # otherwise GRU\n",
        "atten_size = 0          # atten > 0 means using restricted self atten\n",
        "\n",
        "reload_model = True\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "test_interval = 50\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODE = \"MLP\"\n",
        "#MODE = \"RNN\"\n",
        "#MODE = \"GRU\"\n",
        "#MODE = \"MLP_atten\""
      ],
      "metadata": {
        "id": "O7FQa_h75YgC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MAX_LENGTH = 100\n",
        "embedding_size = 100\n",
        "Train_size=30000\n",
        "\n",
        "\n",
        "\n",
        "def review_clean(text):\n",
        "    text = re.sub(r'[^A-Za-z]+', ' ', text)  # remove non alphabetic character\n",
        "    text = re.sub(r'https?:/\\/\\S+', ' ', text)  # remove links\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)  # remove singale char\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def tokinize(s):\n",
        "    s = review_clean(s).lower()\n",
        "    splited = s.split()\n",
        "    return splited[:MAX_LENGTH]\n",
        "\n",
        "\n",
        "def load_data_set(load_my_reviews=False):\n",
        "    data=pd.read_csv(\"/content/IMDB_Dataset.csv\")\n",
        "    train_data=data[:Train_size]\n",
        "    train_iter=ReviewDataset(train_data[\"review\"],train_data[\"sentiment\"])\n",
        "    test_data=data[Train_size:]\n",
        "    if load_my_reviews:\n",
        "        my_data = pd.DataFrame({\"review\": my_test_texts, \"sentiment\": my_test_labels})\n",
        "        # test_data=test_data.append(my_data)\n",
        "        test_data = pd.concat([test_data, my_data], ignore_index=True)\n",
        "    test_data=test_data.reset_index(drop=True)\n",
        "    test_iter=ReviewDataset(test_data[\"review\"],test_data[\"sentiment\"])\n",
        "    return train_iter, test_iter\n",
        "\n",
        "\n",
        "embadding = GloVe(name='6B', dim=embedding_size)\n",
        "tokenizer = get_tokenizer(tokenizer=tokinize)"
      ],
      "metadata": {
        "id": "BNDUyhNI8g8g",
        "outputId": "3381d631-7e6e-4e8c-c33f-53261af67ac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:23<00:00, 17286.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qClslSUJbK0v"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def preprocess_review(s):\n",
        "    cleaned = tokinize(s)\n",
        "    embadded = embadding.get_vecs_by_tokens(cleaned)\n",
        "    if embadded.shape[0] != 100 or embadded.shape[1] != 100:\n",
        "        embadded = torch.nn.functional.pad(embadded, (0, 0, 0, MAX_LENGTH - embadded.shape[0]))\n",
        "    return torch.unsqueeze(embadded, 0)\n",
        "\n",
        "\n",
        "def preprocess_label(label):\n",
        "    return [0.0, 1.0] if label == \"negative\" else [1.0, 0.0]\n",
        "\n",
        "\n",
        "def collact_batch(batch):\n",
        "    label_list = []\n",
        "    review_list = []\n",
        "    embadding_list=[]\n",
        "    for  review,label in batch:\n",
        "        label_list.append(preprocess_label(label))### label\n",
        "        review_list.append(tokinize(review))### the  actuall review\n",
        "        processed_review = preprocess_review(review).detach()\n",
        "        embadding_list.append(processed_review) ### the embedding vectors\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32).reshape((-1, 2))\n",
        "    embadding_tensor= torch.cat(embadding_list)\n",
        "    return label_list.to(device), embadding_tensor.to(device) ,review_list\n",
        "\n",
        "\n",
        "##########################\n",
        "# ADD YOUR OWN TEST TEXT #\n",
        "##########################\n",
        "\n",
        "my_test_texts = []\n",
        "my_test_texts.append(\" Never did I have such a good time in a movie in my whole life! Had such a good time playing on my smartphone the whole movie.\")\n",
        "my_test_texts.append(\" The movie was so captivating that I was completely absorbed in it, causing me to return home late to my wife, who was understandably upset that I lost track of time.\")\n",
        "my_test_texts.append(\"I was so engrossed in the movie's plot twists that I lost track of time and ended up missing an important appointment, bad luck.\")\n",
        "my_test_labels = [\"neg\", \"pos\", \"pos\"]\n",
        "\n",
        "##########################\n",
        "##########################\n",
        "\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, review_list, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.reviews = review_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.reviews[index]\n",
        "        y = self.labels[index]\n",
        "        return X, y\n",
        "\n",
        "\n",
        "\n",
        "def get_data_set(batch_size, toy=False):\n",
        "        train_data, test_data = load_data_set(load_my_reviews=toy)\n",
        "        train_dataloader = DataLoader(train_data, batch_size=batch_size,\n",
        "                                      shuffle=True, collate_fn=collact_batch)\n",
        "        test_dataloader = DataLoader(test_data, batch_size=batch_size,\n",
        "                                     shuffle=True, collate_fn=collact_batch)\n",
        "        return train_dataloader, test_dataloader, MAX_LENGTH, embedding_size\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jGlTFP7ybn6i"
      },
      "outputs": [],
      "source": [
        "# Loading sataset, use toy = True for obtaining a smaller dataset\n",
        "\n",
        "train_dataset, test_dataset, num_words, input_size = get_data_set(batch_size)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4wAuGAaHfc8B"
      },
      "outputs": [],
      "source": [
        "# Special matrix multipication layer (like torch.Linear but can operate on arbitrary sized\n",
        "# tensors and considers its last two indices as the matrix.)\n",
        "\n",
        "class MatMul(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_bias = True):\n",
        "        super(MatMul, self).__init__()\n",
        "        self.matrix = torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(in_channels,out_channels)),\n",
        "                                         requires_grad=True)\n",
        "        if use_bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(1,1,out_channels), requires_grad=True)\n",
        "\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(DEVICE)\n",
        "        x = torch.matmul(x,self.matrix)\n",
        "        if self.use_bias:\n",
        "            x = x+ self.bias\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jfl_-2ILb7Ou"
      },
      "outputs": [],
      "source": [
        "# Implements RNN Unit\n",
        "\n",
        "class ExRNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExRNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sigmoid = torch.sigmoid\n",
        "        self.tanh = torch.tanh\n",
        "\n",
        "        # RNN Cell weights\n",
        "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size).to(DEVICE)\n",
        "        self.output_mlp = nn.Linear(hidden_size,output_size).to(DEVICE)\n",
        "\n",
        "    def name(self):\n",
        "        return \"RNN\"\n",
        "\n",
        "    def forward(self, x, hidden_state): #/1,time,(100,100)\n",
        "\n",
        "        x, hidden_state = x.to(DEVICE), hidden_state.to(DEVICE)\n",
        "        # Implementation of RNN cell\n",
        "        mid = torch.cat((x, hidden_state), 1)\n",
        "        hidden = self.tanh(self.in2hidden(mid))\n",
        "        output =self.output_mlp(hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        return torch.zeros(bs, self.hidden_size).to(DEVICE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5Wqkegnsb_Xl"
      },
      "outputs": [],
      "source": [
        "# Implements GRU Unit\n",
        "\n",
        "class ExGRU(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExGRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sigmoid = torch.sigmoid\n",
        "        self.tanh = torch.tanh\n",
        "        self.update_gate = nn.Linear(input_size+hidden_size,hidden_size).to(DEVICE)\n",
        "        self.reset_gate = nn.Linear(input_size+hidden_size,hidden_size).to(DEVICE)\n",
        "        self.fc = nn.Linear(input_size + hidden_size, hidden_size).to(DEVICE)\n",
        "        self.output_mlp = nn.Linear(hidden_size,output_size).to(DEVICE)\n",
        "\n",
        "    def name(self):\n",
        "        return \"GRU\"\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        x, hidden_state = x.to(DEVICE), hidden_state.to(DEVICE)\n",
        "        cat_tensor = torch.cat([hidden_state,x],dim=1)\n",
        "        zt = self.sigmoid(self.update_gate(cat_tensor))\n",
        "        rt = self.sigmoid(self.reset_gate(cat_tensor))\n",
        "        mid = hidden_state*rt\n",
        "        cat_mid = torch.cat([mid,x], dim=1)\n",
        "        h_hat = self.tanh(self.fc(cat_mid))\n",
        "        hidden = ((1-zt)*hidden_state) + (zt*h_hat)\n",
        "        output = self.output_mlp(hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        return torch.zeros(bs, self.hidden_size).to(DEVICE)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qBR1fJg4cDrN"
      },
      "outputs": [],
      "source": [
        "class ExMLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(ExMLP, self).__init__()\n",
        "\n",
        "        self.ReLU = torch.nn.ReLU()\n",
        "\n",
        "        # Token-wise MLP network weights\n",
        "        self.layer1 = MatMul(input_size,hidden_size).to(DEVICE)\n",
        "        # additional layer(s)\n",
        "        self.layer2 = nn.Linear(hidden_size,hidden_size).to(DEVICE)\n",
        "\n",
        "\n",
        "    def name(self):\n",
        "        return \"MLP\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(DEVICE)\n",
        "        # Token-wise MLP network implementation\n",
        "        x = self.layer1(x)\n",
        "        x = self.ReLU(x)\n",
        "        # rest\n",
        "        x = self.layer2(x)\n",
        "        x = torch.sum(x, dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9gnyQDQncIG6"
      },
      "outputs": [],
      "source": [
        "class ExLRestSelfAtten(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        pass # todo: fix\n",
        "        super(ExLRestSelfAtten, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.sqrt_hidden_size = np.sqrt(float(hidden_size))\n",
        "        self.ReLU = torch.nn.ReLU()\n",
        "        self.softmax = torch.nn.Softmax(2)\n",
        "\n",
        "        # Token-wise MLP + Restricted Attention network implementation\n",
        "\n",
        "        self.layer1 = MatMul(input_size,hidden_size)\n",
        "        self.W_q = MatMul(hidden_size, hidden_size, use_bias=False)\n",
        "        # rest ...\n",
        "\n",
        "\n",
        "    def name(self):\n",
        "        return \"MLP_atten\"\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Token-wise MLP + Restricted Attention network implementation\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.ReLU(x)\n",
        "\n",
        "        # generating x in offsets between -atten_size and atten_size\n",
        "        # with zero padding at the ends\n",
        "\n",
        "        padded = pad(x,(0,0,atten_size,atten_size,0,0))\n",
        "\n",
        "        x_nei = []\n",
        "        for k in range(-atten_size,atten_size+1):\n",
        "            x_nei.append(torch.roll(padded, k, 1))\n",
        "\n",
        "        x_nei = torch.stack(x_nei,2)\n",
        "        x_nei = x_nei[:,atten_size:-atten_size,:]\n",
        "\n",
        "        # x_nei has an additional axis that corresponds to the offset\n",
        "\n",
        "        # Applying attention layer\n",
        "\n",
        "        # query = ...\n",
        "        # keys = ...\n",
        "        # vals = ...\n",
        "\n",
        "\n",
        "        return x, atten_weights\n",
        "\n",
        "\n",
        "# prints portion of the review (20-30 first words), with the sub-scores each work obtained\n",
        "# prints also the final scores, the softmaxed prediction values and the true label values\n",
        "\n",
        "def print_review(rev_text, sbs1, sbs2, lbl1, lbl2):\n",
        "    # Extract a portion of the review text (20-30 first words)\n",
        "    review_portion = ' '.join(rev_text[:30])\n",
        "\n",
        "    # Print review portion\n",
        "    print(f\"Review text (20-30 words): {review_portion}\")\n",
        "\n",
        "    # Print sub-scores for each word\n",
        "    print(f\"Sub-scores for class 1 (negative): {sbs1}\")\n",
        "    print(f\"Sub-scores for class 2 (positive): {sbs2}\")\n",
        "\n",
        "    # Calculate final scores\n",
        "    final_scores = torch.tensor([sbs1, sbs2])\n",
        "\n",
        "    # Softmax to get probabilities\n",
        "    probabilities = F.softmax(final_scores, dim=0)\n",
        "\n",
        "    # Print final scores, softmaxed prediction values, and true label values\n",
        "    print(f\"Final scores: {final_scores}\")\n",
        "    print(f\"Softmaxed predictions: {probabilities}\")\n",
        "    print(f\"True label values: {lbl1}, {lbl2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T1xalwDpcKyn"
      },
      "outputs": [],
      "source": [
        "# @title Run Model Function\n",
        "# select model to use\n",
        "\n",
        "def run_model(results, hidden_size=64, run_recurrent=True, use_RNN=True, reload_model=False, atten_size=0):\n",
        "      if run_recurrent:\n",
        "          if use_RNN:\n",
        "              model = ExRNN(input_size, output_size, hidden_size)\n",
        "          else:\n",
        "              model = ExGRU(input_size, output_size, hidden_size)\n",
        "      else:\n",
        "          if atten_size > 0:\n",
        "              model = ExLRestSelfAtten(input_size, output_size, hidden_size)\n",
        "          else:\n",
        "              model = ExMLP(input_size, output_size, hidden_size)\n",
        "\n",
        "      print(\"Using model: \" + model.name())\n",
        "\n",
        "      if reload_model:\n",
        "          print(\"Reloading model\")\n",
        "          model.load_state_dict(torch.load(model.name() + \".pth\"))\n",
        "\n",
        "      # move model to GPU if avilavble\n",
        "      model.to(DEVICE)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      train_loss = 1.0\n",
        "      test_loss = 1.0\n",
        "\n",
        "      train_losses = []\n",
        "      test_losses = []\n",
        "      accuracies = []\n",
        "\n",
        "      # training steps in which a test step is executed every test_interval\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "\n",
        "          itr = 0 # iteration counter within each epoch\n",
        "\n",
        "          for labels, reviews, reviews_text in train_dataset:   # getting training batches\n",
        "\n",
        "              # print(f\"FOR DEBBUGING:: Labels shape: {labels.shape}\")\n",
        "              # print(f\"FOR DEBBUGING:: Label: {labels}\")\n",
        "\n",
        "              itr = itr + 1\n",
        "\n",
        "              if (itr + 1) % test_interval == 0:\n",
        "                  test_iter = True\n",
        "                  labels, reviews, reviews_text = next(iter(test_dataset)) # get a test batch\n",
        "              else:\n",
        "                  test_iter = False\n",
        "\n",
        "              # Recurrent nets (RNN/GRU)\n",
        "\n",
        "              if run_recurrent:\n",
        "                  hidden_state = model.init_hidden(int(labels.shape[0]))\n",
        "\n",
        "                  for i in range(num_words):\n",
        "                      output, hidden_state = model(reviews[:,i,:], hidden_state)\n",
        "\n",
        "              else:\n",
        "\n",
        "              # Token-wise networks (MLP / MLP + Atten.)\n",
        "\n",
        "                  sub_score = []\n",
        "                  if atten_size > 0:\n",
        "                      # MLP + atten\n",
        "                      sub_score, atten_weights = model(reviews)\n",
        "                  else:\n",
        "                      # MLP\n",
        "                      sub_score = model(reviews)\n",
        "\n",
        "                  output = torch.mean(sub_score, 1)\n",
        "\n",
        "                  # print(f\"FOR DEBBUGING:: Output shape: {output.shape}, Labels shape: {labels.shape}\")\n",
        "                  if model.name() == \"MLP\":\n",
        "                      labels = torch.argmax(labels, dim=1)\n",
        "                      labels = labels.to(torch.float32)\n",
        "                      labels.to(DEVICE)\n",
        "                      # print(f\"FOR DEBBUGING:: Label: {labels}\")\n",
        "\n",
        "              # cross-entropy loss\n",
        "\n",
        "              loss = criterion(output, labels)\n",
        "\n",
        "              # optimize in training iterations\n",
        "\n",
        "              if not test_iter:\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "              # averaged losses\n",
        "              if test_iter:\n",
        "                  test_loss = 0.8 * float(loss.detach()) + 0.2 * test_loss\n",
        "              else:\n",
        "                  train_loss = 0.9 * float(loss.detach()) + 0.1 * train_loss\n",
        "\n",
        "              if test_iter:\n",
        "                train_losses.append(train_loss)\n",
        "                test_losses.append(test_loss)\n",
        "\n",
        "\n",
        "                # Calculate accuracy\n",
        "                # Ensure the output and labels have compatible dimensions\n",
        "                #print(f\"Output shape before processing: {output.shape}\")\n",
        "                #print(f\"Labels shape before processing: {labels.shape}\")\n",
        "\n",
        "                if len(output.shape) > 1 and output.shape[1] > 1:\n",
        "                    _, predicted = torch.max(output, 1)\n",
        "                else:\n",
        "                    predicted = output\n",
        "\n",
        "                # Convert labels to class indices if they are not already\n",
        "                if len(labels.shape) > 1 and labels.shape[1] > 1:\n",
        "                    labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "                # Print shapes after processing\n",
        "                #print(f\"Output shape after processing: {output.shape}\")\n",
        "                #print(f\"Predicted shape: {predicted.shape}\")\n",
        "                #print(f\"Labels shape after processing: {labels.shape}\")\n",
        "\n",
        "                # Print values to debug\n",
        "                #print(f\"Output: {output}\")\n",
        "                #print(f\"Predicted: {predicted}\")\n",
        "                #print(f\"Labels: {labels}\")\n",
        "\n",
        "                correct = (predicted == labels).sum().item()\n",
        "                accuracy = correct / labels.size(0)\n",
        "                accuracies.append(accuracy)\n",
        "\n",
        "                # Print correct predictions and accuracy\n",
        "                #print(f\"Correct predictions: {correct}\")\n",
        "                #print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "                print(\n",
        "                      f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "                      f\"Step [{itr + 1}/{len(train_dataset)}], \"\n",
        "                      f\"Train Loss: {train_loss:.4f}, \"\n",
        "                      f\"Test Loss: {test_loss:.4f}\"\n",
        "                  )\n",
        "\n",
        "                if not run_recurrent and reload_model: # don't prinr preview in train\n",
        "                    nump_subs = sub_score.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    print_review(reviews_text[0], nump_subs[0], nump_subs[1], labels[0], labels[1])\n",
        "\n",
        "                  # saving the model\n",
        "                torch.save(model, model.name() + \".pth\")\n",
        "      results = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"accuracies\": accuracies\n",
        "      }\n",
        "\n",
        "      return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7jeZSUwT8RLv",
        "outputId": "5bd608eb-e71b-43dc-b2f1-3cd66715efd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model: MLP\n",
            "Epoch [1/10], Step [50/938], Train Loss: 47.9626, Test Loss: 31.5902\n",
            "Epoch [1/10], Step [100/938], Train Loss: 39.0839, Test Loss: 45.3888\n",
            "Epoch [1/10], Step [150/938], Train Loss: 57.9602, Test Loss: 49.7720\n",
            "Epoch [1/10], Step [200/938], Train Loss: 35.6862, Test Loss: 49.6095\n",
            "Epoch [1/10], Step [250/938], Train Loss: 38.2480, Test Loss: 57.6064\n",
            "Epoch [1/10], Step [300/938], Train Loss: 64.5071, Test Loss: 48.7661\n",
            "Epoch [1/10], Step [350/938], Train Loss: 59.4570, Test Loss: 32.4287\n",
            "Epoch [1/10], Step [400/938], Train Loss: 51.7095, Test Loss: 36.5872\n",
            "Epoch [1/10], Step [450/938], Train Loss: 58.7373, Test Loss: 52.8906\n",
            "Epoch [1/10], Step [500/938], Train Loss: 42.2301, Test Loss: 57.5773\n",
            "Epoch [1/10], Step [550/938], Train Loss: 46.1389, Test Loss: 44.6670\n",
            "Epoch [1/10], Step [600/938], Train Loss: 44.7617, Test Loss: 46.0143\n",
            "Epoch [1/10], Step [650/938], Train Loss: 49.8106, Test Loss: 49.1451\n",
            "Epoch [1/10], Step [700/938], Train Loss: 35.1072, Test Loss: 39.9918\n",
            "Epoch [1/10], Step [750/938], Train Loss: 55.6247, Test Loss: 64.3352\n",
            "Epoch [1/10], Step [800/938], Train Loss: 52.7400, Test Loss: 69.6897\n",
            "Epoch [1/10], Step [850/938], Train Loss: 66.4235, Test Loss: 61.3991\n",
            "Epoch [1/10], Step [900/938], Train Loss: 48.1516, Test Loss: 39.9113\n",
            "Epoch [2/10], Step [50/938], Train Loss: 38.0901, Test Loss: 46.0280\n",
            "Epoch [2/10], Step [100/938], Train Loss: 49.8369, Test Loss: 43.2990\n",
            "Epoch [2/10], Step [150/938], Train Loss: 48.6485, Test Loss: 49.5254\n",
            "Epoch [2/10], Step [200/938], Train Loss: 57.3299, Test Loss: 36.6212\n",
            "Epoch [2/10], Step [250/938], Train Loss: 70.3774, Test Loss: 51.5017\n",
            "Epoch [2/10], Step [300/938], Train Loss: 46.1377, Test Loss: 60.4654\n",
            "Epoch [2/10], Step [350/938], Train Loss: 59.6768, Test Loss: 54.7423\n",
            "Epoch [2/10], Step [400/938], Train Loss: 65.1183, Test Loss: 58.3149\n",
            "Epoch [2/10], Step [450/938], Train Loss: 42.0195, Test Loss: 50.6527\n",
            "Epoch [2/10], Step [500/938], Train Loss: 46.1672, Test Loss: 40.4605\n",
            "Epoch [2/10], Step [550/938], Train Loss: 48.9581, Test Loss: 55.9296\n",
            "Epoch [2/10], Step [600/938], Train Loss: 52.8842, Test Loss: 56.4448\n",
            "Epoch [2/10], Step [650/938], Train Loss: 56.4466, Test Loss: 49.6182\n",
            "Epoch [2/10], Step [700/938], Train Loss: 49.4280, Test Loss: 65.5743\n",
            "Epoch [2/10], Step [750/938], Train Loss: 64.6472, Test Loss: 67.3246\n",
            "Epoch [2/10], Step [800/938], Train Loss: 47.8509, Test Loss: 56.9482\n",
            "Epoch [2/10], Step [850/938], Train Loss: 49.7124, Test Loss: 53.0987\n",
            "Epoch [2/10], Step [900/938], Train Loss: 41.4718, Test Loss: 52.1925\n",
            "Epoch [3/10], Step [50/938], Train Loss: 47.8168, Test Loss: 51.5266\n",
            "Epoch [3/10], Step [100/938], Train Loss: 51.6147, Test Loss: 58.9926\n",
            "Epoch [3/10], Step [150/938], Train Loss: 37.7048, Test Loss: 53.2243\n",
            "Epoch [3/10], Step [200/938], Train Loss: 55.7585, Test Loss: 52.3782\n",
            "Epoch [3/10], Step [250/938], Train Loss: 58.2117, Test Loss: 55.4223\n",
            "Epoch [3/10], Step [300/938], Train Loss: 38.5019, Test Loss: 57.0034\n",
            "Epoch [3/10], Step [350/938], Train Loss: 55.1906, Test Loss: 57.8780\n",
            "Epoch [3/10], Step [400/938], Train Loss: 58.7067, Test Loss: 53.1868\n",
            "Epoch [3/10], Step [450/938], Train Loss: 41.6808, Test Loss: 49.7633\n",
            "Epoch [3/10], Step [500/938], Train Loss: 67.7319, Test Loss: 35.3175\n",
            "Epoch [3/10], Step [550/938], Train Loss: 53.6654, Test Loss: 49.1286\n",
            "Epoch [3/10], Step [600/938], Train Loss: 50.2368, Test Loss: 57.3317\n",
            "Epoch [3/10], Step [650/938], Train Loss: 36.2254, Test Loss: 65.2241\n",
            "Epoch [3/10], Step [700/938], Train Loss: 48.5734, Test Loss: 53.8372\n",
            "Epoch [3/10], Step [750/938], Train Loss: 49.5225, Test Loss: 50.4490\n",
            "Epoch [3/10], Step [800/938], Train Loss: 53.2736, Test Loss: 54.6839\n",
            "Epoch [3/10], Step [850/938], Train Loss: 45.8352, Test Loss: 45.8558\n",
            "Epoch [3/10], Step [900/938], Train Loss: 44.6637, Test Loss: 43.9051\n",
            "Epoch [4/10], Step [50/938], Train Loss: 71.7260, Test Loss: 46.7022\n",
            "Epoch [4/10], Step [100/938], Train Loss: 64.8958, Test Loss: 52.9960\n",
            "Epoch [4/10], Step [150/938], Train Loss: 50.2211, Test Loss: 53.9749\n",
            "Epoch [4/10], Step [200/938], Train Loss: 70.6437, Test Loss: 40.2659\n",
            "Epoch [4/10], Step [250/938], Train Loss: 60.9121, Test Loss: 43.2416\n",
            "Epoch [4/10], Step [300/938], Train Loss: 47.6458, Test Loss: 51.0891\n",
            "Epoch [4/10], Step [350/938], Train Loss: 59.2767, Test Loss: 61.6364\n",
            "Epoch [4/10], Step [400/938], Train Loss: 61.6861, Test Loss: 57.3918\n",
            "Epoch [4/10], Step [450/938], Train Loss: 56.0408, Test Loss: 61.7307\n",
            "Epoch [4/10], Step [500/938], Train Loss: 49.1108, Test Loss: 47.2931\n",
            "Epoch [4/10], Step [550/938], Train Loss: 64.7696, Test Loss: 43.2565\n",
            "Epoch [4/10], Step [600/938], Train Loss: 42.5228, Test Loss: 38.7518\n",
            "Epoch [4/10], Step [650/938], Train Loss: 39.9097, Test Loss: 37.5312\n",
            "Epoch [4/10], Step [700/938], Train Loss: 48.3897, Test Loss: 43.2811\n",
            "Epoch [4/10], Step [750/938], Train Loss: 26.0645, Test Loss: 28.7811\n",
            "Epoch [4/10], Step [800/938], Train Loss: 74.6537, Test Loss: 51.3263\n",
            "Epoch [4/10], Step [850/938], Train Loss: 54.2141, Test Loss: 48.9750\n",
            "Epoch [4/10], Step [900/938], Train Loss: 62.3778, Test Loss: 39.8720\n",
            "Epoch [5/10], Step [50/938], Train Loss: 43.3798, Test Loss: 45.3407\n",
            "Epoch [5/10], Step [100/938], Train Loss: 53.4748, Test Loss: 55.2625\n",
            "Epoch [5/10], Step [150/938], Train Loss: 53.3689, Test Loss: 49.8351\n",
            "Epoch [5/10], Step [200/938], Train Loss: 37.7257, Test Loss: 48.9940\n",
            "Epoch [5/10], Step [250/938], Train Loss: 29.8740, Test Loss: 31.8343\n",
            "Epoch [5/10], Step [300/938], Train Loss: 45.4580, Test Loss: 41.7220\n",
            "Epoch [5/10], Step [350/938], Train Loss: 46.9420, Test Loss: 40.1997\n",
            "Epoch [5/10], Step [400/938], Train Loss: 56.7879, Test Loss: 42.1266\n",
            "Epoch [5/10], Step [450/938], Train Loss: 55.8237, Test Loss: 46.1968\n",
            "Epoch [5/10], Step [500/938], Train Loss: 45.7014, Test Loss: 42.0690\n",
            "Epoch [5/10], Step [550/938], Train Loss: 56.9096, Test Loss: 49.2362\n",
            "Epoch [5/10], Step [600/938], Train Loss: 37.7343, Test Loss: 42.4054\n",
            "Epoch [5/10], Step [650/938], Train Loss: 40.6026, Test Loss: 66.3975\n",
            "Epoch [5/10], Step [700/938], Train Loss: 53.0818, Test Loss: 52.6218\n",
            "Epoch [5/10], Step [750/938], Train Loss: 53.8117, Test Loss: 45.3448\n",
            "Epoch [5/10], Step [800/938], Train Loss: 55.1505, Test Loss: 54.2816\n",
            "Epoch [5/10], Step [850/938], Train Loss: 56.7434, Test Loss: 52.0422\n",
            "Epoch [5/10], Step [900/938], Train Loss: 52.2299, Test Loss: 55.2160\n",
            "Epoch [6/10], Step [50/938], Train Loss: 48.4074, Test Loss: 61.9300\n",
            "Epoch [6/10], Step [100/938], Train Loss: 43.7665, Test Loss: 42.5911\n",
            "Epoch [6/10], Step [150/938], Train Loss: 57.3056, Test Loss: 43.7875\n",
            "Epoch [6/10], Step [200/938], Train Loss: 58.5722, Test Loss: 62.7621\n",
            "Epoch [6/10], Step [250/938], Train Loss: 60.0242, Test Loss: 54.1455\n",
            "Epoch [6/10], Step [300/938], Train Loss: 66.5229, Test Loss: 46.6263\n",
            "Epoch [6/10], Step [350/938], Train Loss: 52.2448, Test Loss: 61.6931\n",
            "Epoch [6/10], Step [400/938], Train Loss: 68.3324, Test Loss: 63.3605\n",
            "Epoch [6/10], Step [450/938], Train Loss: 61.6019, Test Loss: 73.5380\n",
            "Epoch [6/10], Step [500/938], Train Loss: 42.6051, Test Loss: 44.4134\n",
            "Epoch [6/10], Step [550/938], Train Loss: 53.6864, Test Loss: 44.1569\n",
            "Epoch [6/10], Step [600/938], Train Loss: 55.7615, Test Loss: 44.5211\n",
            "Epoch [6/10], Step [650/938], Train Loss: 45.2145, Test Loss: 57.4346\n",
            "Epoch [6/10], Step [700/938], Train Loss: 47.8658, Test Loss: 52.8474\n",
            "Epoch [6/10], Step [750/938], Train Loss: 46.5126, Test Loss: 48.9602\n",
            "Epoch [6/10], Step [800/938], Train Loss: 57.5136, Test Loss: 44.5369\n",
            "Epoch [6/10], Step [850/938], Train Loss: 64.2444, Test Loss: 56.8587\n",
            "Epoch [6/10], Step [900/938], Train Loss: 30.4990, Test Loss: 37.5025\n",
            "Epoch [7/10], Step [50/938], Train Loss: 56.2074, Test Loss: 29.6331\n",
            "Epoch [7/10], Step [100/938], Train Loss: 41.9982, Test Loss: 47.4841\n",
            "Epoch [7/10], Step [150/938], Train Loss: 48.2498, Test Loss: 51.3993\n",
            "Epoch [7/10], Step [200/938], Train Loss: 31.2756, Test Loss: 46.8449\n",
            "Epoch [7/10], Step [250/938], Train Loss: 44.7467, Test Loss: 42.0242\n",
            "Epoch [7/10], Step [300/938], Train Loss: 31.4326, Test Loss: 55.1329\n",
            "Epoch [7/10], Step [350/938], Train Loss: 49.1046, Test Loss: 47.2516\n",
            "Epoch [7/10], Step [400/938], Train Loss: 55.3084, Test Loss: 41.3487\n",
            "Epoch [7/10], Step [450/938], Train Loss: 50.7085, Test Loss: 65.0106\n",
            "Epoch [7/10], Step [500/938], Train Loss: 47.6172, Test Loss: 48.3908\n",
            "Epoch [7/10], Step [550/938], Train Loss: 52.7003, Test Loss: 44.5833\n",
            "Epoch [7/10], Step [600/938], Train Loss: 39.9666, Test Loss: 59.2120\n",
            "Epoch [7/10], Step [650/938], Train Loss: 53.7217, Test Loss: 66.5990\n",
            "Epoch [7/10], Step [700/938], Train Loss: 63.3594, Test Loss: 52.4471\n",
            "Epoch [7/10], Step [750/938], Train Loss: 46.3045, Test Loss: 68.3251\n",
            "Epoch [7/10], Step [800/938], Train Loss: 51.3839, Test Loss: 56.7589\n",
            "Epoch [7/10], Step [850/938], Train Loss: 72.8847, Test Loss: 63.2311\n",
            "Epoch [7/10], Step [900/938], Train Loss: 61.2514, Test Loss: 50.9807\n",
            "Epoch [8/10], Step [50/938], Train Loss: 66.9682, Test Loss: 61.9761\n",
            "Epoch [8/10], Step [100/938], Train Loss: 54.2030, Test Loss: 49.8003\n",
            "Epoch [8/10], Step [150/938], Train Loss: 53.0826, Test Loss: 54.1491\n",
            "Epoch [8/10], Step [200/938], Train Loss: 34.1430, Test Loss: 49.5687\n",
            "Epoch [8/10], Step [250/938], Train Loss: 57.1146, Test Loss: 46.1118\n",
            "Epoch [8/10], Step [300/938], Train Loss: 55.0740, Test Loss: 56.2513\n",
            "Epoch [8/10], Step [350/938], Train Loss: 47.8265, Test Loss: 63.3456\n",
            "Epoch [8/10], Step [400/938], Train Loss: 57.1180, Test Loss: 38.8526\n",
            "Epoch [8/10], Step [450/938], Train Loss: 49.2348, Test Loss: 54.7787\n",
            "Epoch [8/10], Step [500/938], Train Loss: 56.3699, Test Loss: 41.3455\n",
            "Epoch [8/10], Step [550/938], Train Loss: 49.6525, Test Loss: 44.4518\n",
            "Epoch [8/10], Step [600/938], Train Loss: 51.8736, Test Loss: 42.3459\n",
            "Epoch [8/10], Step [650/938], Train Loss: 45.7970, Test Loss: 52.5517\n",
            "Epoch [8/10], Step [700/938], Train Loss: 43.0241, Test Loss: 40.9575\n",
            "Epoch [8/10], Step [750/938], Train Loss: 40.8664, Test Loss: 49.5773\n",
            "Epoch [8/10], Step [800/938], Train Loss: 47.6948, Test Loss: 70.9267\n",
            "Epoch [8/10], Step [850/938], Train Loss: 59.9997, Test Loss: 66.9763\n",
            "Epoch [8/10], Step [900/938], Train Loss: 50.7649, Test Loss: 46.7214\n",
            "Epoch [9/10], Step [50/938], Train Loss: 49.6630, Test Loss: 47.3960\n",
            "Epoch [9/10], Step [100/938], Train Loss: 38.0905, Test Loss: 55.9184\n",
            "Epoch [9/10], Step [150/938], Train Loss: 48.1008, Test Loss: 60.4552\n",
            "Epoch [9/10], Step [200/938], Train Loss: 40.6915, Test Loss: 57.9388\n",
            "Epoch [9/10], Step [250/938], Train Loss: 36.5485, Test Loss: 57.9696\n",
            "Epoch [9/10], Step [300/938], Train Loss: 44.5721, Test Loss: 84.4832\n",
            "Epoch [9/10], Step [350/938], Train Loss: 55.0542, Test Loss: 63.7798\n",
            "Epoch [9/10], Step [400/938], Train Loss: 54.3541, Test Loss: 51.7484\n",
            "Epoch [9/10], Step [450/938], Train Loss: 59.3221, Test Loss: 43.8311\n",
            "Epoch [9/10], Step [500/938], Train Loss: 59.5422, Test Loss: 58.7389\n",
            "Epoch [9/10], Step [550/938], Train Loss: 43.3148, Test Loss: 60.1262\n",
            "Epoch [9/10], Step [600/938], Train Loss: 55.1112, Test Loss: 58.5931\n",
            "Epoch [9/10], Step [650/938], Train Loss: 46.2458, Test Loss: 41.8422\n",
            "Epoch [9/10], Step [700/938], Train Loss: 50.1917, Test Loss: 49.7227\n",
            "Epoch [9/10], Step [750/938], Train Loss: 64.0396, Test Loss: 51.9180\n",
            "Epoch [9/10], Step [800/938], Train Loss: 49.4676, Test Loss: 55.5022\n",
            "Epoch [9/10], Step [850/938], Train Loss: 72.1104, Test Loss: 62.7079\n",
            "Epoch [9/10], Step [900/938], Train Loss: 50.8386, Test Loss: 47.5263\n",
            "Epoch [10/10], Step [50/938], Train Loss: 51.4584, Test Loss: 52.4880\n",
            "Epoch [10/10], Step [100/938], Train Loss: 66.4650, Test Loss: 54.4866\n",
            "Epoch [10/10], Step [150/938], Train Loss: 61.2310, Test Loss: 49.0570\n",
            "Epoch [10/10], Step [200/938], Train Loss: 43.7622, Test Loss: 49.8618\n",
            "Epoch [10/10], Step [250/938], Train Loss: 55.4704, Test Loss: 46.5159\n",
            "Epoch [10/10], Step [300/938], Train Loss: 54.7486, Test Loss: 46.9440\n",
            "Epoch [10/10], Step [350/938], Train Loss: 52.6517, Test Loss: 50.2682\n",
            "Epoch [10/10], Step [400/938], Train Loss: 51.0005, Test Loss: 57.8818\n",
            "Epoch [10/10], Step [450/938], Train Loss: 57.4970, Test Loss: 56.7856\n",
            "Epoch [10/10], Step [500/938], Train Loss: 58.5169, Test Loss: 65.0776\n",
            "Epoch [10/10], Step [550/938], Train Loss: 40.6816, Test Loss: 49.0311\n",
            "Epoch [10/10], Step [600/938], Train Loss: 27.6099, Test Loss: 50.8167\n",
            "Epoch [10/10], Step [650/938], Train Loss: 46.2645, Test Loss: 40.3373\n",
            "Epoch [10/10], Step [700/938], Train Loss: 43.0642, Test Loss: 58.8022\n",
            "Epoch [10/10], Step [750/938], Train Loss: 37.4395, Test Loss: 64.0020\n",
            "Epoch [10/10], Step [800/938], Train Loss: 45.5537, Test Loss: 29.4102\n",
            "Epoch [10/10], Step [850/938], Train Loss: 45.8661, Test Loss: 55.4613\n",
            "Epoch [10/10], Step [900/938], Train Loss: 41.5444, Test Loss: 53.1505\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'train_losses'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-8e0eaf0b8396>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_recurrent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_recurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_RNN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_RNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matten_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matten_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracies'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'train_losses'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    results = {}\n",
        "    hidden_sizes = [64, 128]\n",
        "    for hidden_size in hidden_sizes:\n",
        "        results[hidden_size] = run_model(results, hidden_size=hidden_size, run_recurrent=run_recurrent, use_RNN=use_RNN, reload_model=reload_model, atten_size=atten_size)\n",
        "        plt.figure()\n",
        "        plt.plot(results[hidden_size]['train_losses'], label='Train Loss')\n",
        "        plt.plot(results[hidden_size]['test_losses'], label='Test Loss')\n",
        "        plt.plot(results[hidden_size]['accuracies'], label='Accuracy')\n",
        "        plt.title(f'Performance with hidden size of {hidden_size}')\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Loss/Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IR3wVqoaciAf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "d305f83c-c384-48e9-c7c0-fee4a8e9ac78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[79.87416  77.12515  69.37811  65.93113  69.69392  80.95571  69.351074\n",
            "  74.74272  68.603905 69.75887  73.839615 84.694084 81.30048  70.348114\n",
            "  68.18561  70.47383  71.09858  74.94895  84.44992  78.81839  71.19244\n",
            "  81.791306 65.64942  72.84935  83.37217  65.454796 65.78671  65.154724\n",
            "  78.87431  74.85554  73.90263  65.66325  84.424805 80.14343  72.11169\n",
            "  77.56879  67.77528  69.822876 68.93255  61.87874  84.751205 74.049164\n",
            "  71.344795 70.73912  79.33252  82.92139  83.3423   83.68999  71.2025\n",
            "  81.14024  80.11383  72.35251  83.63968  67.9016   81.466736 70.08443\n",
            "  73.33458  64.50959  68.80419  84.809944 65.14062  81.6607   79.61394\n",
            "  75.19978 ]]\n",
            "tensor([[[ 0.2831,  0.3328,  0.5000,  ...,  0.0315, -0.2246,  0.4851],\n",
            "         [ 0.3045, -0.1963,  0.2023,  ..., -0.1839, -0.1243,  0.2747],\n",
            "         [ 0.1571,  0.6561,  0.0021,  ..., -0.6061,  0.7100,  0.4147],\n",
            "         ...,\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
            "neg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'output' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b3de05579180>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# We don't need test_dataset here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-b3de05579180>\u001b[0m in \u001b[0;36minference_model\u001b[0;34m(model_path, test_dataset, run_recurrent, num_words)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Get the predicted label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pos\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"neg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         results.append({\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'output' referenced before assignment"
          ]
        }
      ],
      "source": [
        "def inference_model(model_path, test_dataset, run_recurrent=True, num_words=None):\n",
        "    # Load the model\n",
        "    model = torch.load(model_path)\n",
        "    model.eval()\n",
        "    if model.name() == \"MLP\" or model.name() == \"MLP_atten\":\n",
        "        pass\n",
        "    results = []\n",
        "\n",
        "    for review, label in zip(my_test_texts, my_test_labels):\n",
        "        # Preprocess the review\n",
        "        processed_review = preprocess_review(review)\n",
        "\n",
        "        # Initialize hidden state\n",
        "        if 'MLP' not in model_path:\n",
        "          hidden = model.init_hidden(1)  # Batch size is 1 for inference\n",
        "\n",
        "        # Run inference\n",
        "          with torch.no_grad():\n",
        "              output = None\n",
        "              for i in range(processed_review.size(1)):  # Iterate over time steps\n",
        "                  input_step = processed_review[:, i, :]\n",
        "                  output, hidden = model(input_step, hidden)\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "              # todo: fix this\n",
        "              print(\"up to here\")\n",
        "              pass\n",
        "\n",
        "\n",
        "\n",
        "        # Get the predicted label\n",
        "        predicted_label = \"pos\" if output[0][0] > output[0][1] else \"neg\"\n",
        "\n",
        "        results.append({\n",
        "            \"review\": review,\n",
        "            \"true_label\": label,\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"confidence\": max(output[0]).item()\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to print results\n",
        "def print_results(model_name, results):\n",
        "    print(f\"\\nResults for {model_name}:\")\n",
        "    for result in results:\n",
        "        print(f\"Review: {result['review'][:150]}...\")\n",
        "        print(f\"True label: {result['true_label']}\")\n",
        "        print(f\"Predicted label: {result['predicted_label']}\")\n",
        "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "        print()\n",
        "\n",
        "# Run inference for each model\n",
        "model_paths = [\n",
        "    # \"RNN.pth\",\n",
        "    # \"GRU.pth\",\n",
        "    \"MLP.pth\",\n",
        "    # \"MLP_atten.pth\"\n",
        "]\n",
        "\n",
        "for model_path in model_paths:\n",
        "    results = inference_model(model_path, None)  # We don't need test_dataset here\n",
        "    print_results(model_path, results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jK3qB-6qlA5S",
        "outputId": "93c204f1-c84f-4e39-aad4-b396fa5f42fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review type: <class 'ellipsis'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-df9f9fcf1500>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mrun_recurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RNN\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"GRU\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m  \u001b[0;31m# Determine the model type based on the name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_recurrent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_recurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-df9f9fcf1500>\u001b[0m in \u001b[0;36minference_model\u001b[0;34m(model_path, test_texts, test_labels, run_recurrent)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Preprocess the review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"review type: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mprocessed_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-9346c42e90dd>\u001b[0m in \u001b[0;36mpreprocess_review\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokinize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0membadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membadding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vecs_by_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0membadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0membadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0membadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a79cd071ad00>\u001b[0m in \u001b[0;36mtokinize\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokinize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0msplited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msplited\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a79cd071ad00>\u001b[0m in \u001b[0;36mreview_clean\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreview_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^A-Za-z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove non alphabetic character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'https?:/\\/\\S+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\s+[a-zA-Z]\\s+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove singale char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVckZkdnKKJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}